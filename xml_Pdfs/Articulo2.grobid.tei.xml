<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-02-29">29 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<postCode>9X 6.7X</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoqi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<postCode>9X 6.7X</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<postCode>9X 6.7X</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<postCode>9X 6.7X</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<postCode>9X 6.7X</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-29">29 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">4B303A8AF0AB0D1037A344086E7F99B0</idno>
					<idno type="arXiv">arXiv:2402.19299v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-04T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate taskspecific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a twolevel hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks. GPT RL RL-GPT (Ours) Optimized Neural Network Optimized Coded Actions Optimized Actions + Neural Network</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building agents to master tasks in open-world environments has been a long-standing goal in AI research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. The emergence of Large Language Models (LLMs) has revitalized this pursuit, leveraging their expansive world knowledge and adept compositional reasoning capabilities <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>. LLMs agents showcase proficiency in utilizing computer tools <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>, navigating search engines <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, and even operating systems or applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. However, their performance remains constrained in open-world embodied environments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38]</ref>. Despite possessing "world knowledge" akin to a human professor, LLMs fall short when pitted against a child in a video game. The inherent limitation lies in LLMs' adeptness at absorbing information but their inability to practice skills within an environment. Proficiency Figure <ref type="figure">1</ref>. The overview of RL-GPT. After the optimization in an environment, LLMs agents obtain optimized coded actions, RL achieves an optimized neural network, and our RL-GPT gets both optimized coded actions and neural networks. Our framework integrates the coding parts and the learning parts. in activities such as playing a video game demands extensive practice, a facet not easily addressed by in-context learning, which exhibits a relatively low upper bound <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47]</ref>. Consequently, existing LLMs necessitate human intervention to define low-level skills or tools <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Reinforcement Learning (RL), proven as an effective method for learning from interaction, holds promise in facilitating LLMs to "practise". One line of works grounds LLMs for open-world control through RL fine-tuning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53]</ref>. Nevertheless, this approach necessitates a substantial volume of domain-specific data, expert demonstrations, and access to LLMs' parameters, rendering it slow and resource-intensive in most scenarios. Given the modest learning efficiency, the majority of methods continue to operate within the realm of "word games" such as tone adjustment rather than tackling intricate embodied tasks.</p><p>Addressing this challenge, we propose to integrate LLMs and RL in a novel approach: Empower LLMs agents to use an RL training pipeline as a tool. We introduce RL-GPT, a framework designed to enhance LLMs with trainable modules for learning interaction tasks within an environment.</p><p>As shown in Fig. <ref type="figure">3</ref>, RL-GPT comprises an agent pipeline featuring multiple LLMs, wherein the neural network is conceptualized as a tool for training the RL pipeline. Illustrated in Fig. <ref type="figure">1</ref>, unlike conventional approaches where LLMs agents and RL optimize coded actions and networks separately, RL-GPT unifies this optimization process. The line chart in Fig. <ref type="figure">1</ref> illustrates that RL-GPT outperforms alternative approaches on the "harvest a log" task in MineDojo <ref type="bibr" target="#b6">[7]</ref>.</p><p>We further point out that the pivotal issue in using RL is to decide: Which actions should be learned with RL? To tackle this, RL-GPT is meticulously designed to assign different actions to RL and Code-as-policy, respectively. Our agent pipeline entails two fundamental steps. Firstly, LLMs should determine "which actions" to code, involving task decomposition into distinct sub-actions and deciding which actions can be effectively coded. Actions falling outside this realm will be learned through RL. Secondly, LLMs are tasked with writing accurate codes for the "coded actions" and test them in the environment.</p><p>We employ a two-level hierarchical framework to realize the two steps, as depicted in Fig. <ref type="figure">3</ref>. Allocating these steps to two independent agents proves highly effective, as it narrows down the scope of each LLM's task. Coded actions with explicit starting conditions are executed sequentially, while other coded actions are integrated into the RL action space. This strategic insertion into the action space empowers LLMs to make pivotal decisions during the learning process. Illustrated in Fig. <ref type="figure">2</ref>, this integration enhances the efficiency of learning tasks, exemplified by our ability to more effectively learn how to break a tree.</p><p>For intricate tasks such as the ObtainDiamond task in the Minecraft game, devising a strategy with a single neural network proves challenging due to limited computing resources. In response, we incorporate a task planner to facilitate task decomposition. Our RL-GPT framework demonstrates remarkable efficiency in tackling complex embodied tasks. Specifically, within the MineDojo environment, it attains state-of-the-art performance on the majority of selected tasks and adeptly obtains diamonds within a single day, utilizing only an RTX3090 GPU.</p><p>Our contributions are summarized as follows:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Space Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Network</head><p>Figure <ref type="figure">2</ref>. To learn a subtask, the LLM can generate environment configurations (task, observation, reward, and action space) to instantiate RL. In particular, by reasoning about the agent behavior to solve the subtask, the LLM generates code to provide higher-level actions in addition to the original environment actions, improving the sample efficiency for RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Agents in Minecraft</head><p>Minecraft, a widely popular open-world sandbox game, stands as a formidable benchmark for constructing efficient and generalized agents. Previous endeavors resort to hierarchical reinforcement learning, often relying on human demonstrations to facilitate the training of low-level policies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. Efforts such as MineAgent <ref type="bibr" target="#b6">[7]</ref>, Steve-1 <ref type="bibr" target="#b22">[23]</ref>, and VPT <ref type="bibr" target="#b2">[3]</ref> leverage large-scale pre-training via YouTube videos to enhance policy training efficiency. However, MineAgent and Steve-1 are limited to completing only a few short-term tasks, and others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50]</ref> still require a substantial number of steps for finetuning on long-horizon tasks. DreamerV3 <ref type="bibr" target="#b12">[13]</ref> utilizes a world model to expedite exploration but still demands a substantial number of interactions to acquire diamonds. These existing approaches either necessitate extensive expert datasets for training or exhibit low sample efficiency when addressing long-horizon tasks in the Minecraft environment. An alternative research direction employs Large Language Models (LLMs) for task decomposition and high-level planning to address intricate challenges. Certain works <ref type="bibr" target="#b36">[37]</ref> leverage few-shot prompting with Codex <ref type="bibr" target="#b4">[5]</ref> to generate executable policies. DEPS <ref type="bibr" target="#b41">[42]</ref> and GITM <ref type="bibr" target="#b54">[55]</ref> investigate the use of LLMs as high-level planners in the Minecraft context. Some works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51]</ref> further explore LLMs for high-level planning, code generation, lifelong exploration, and creative tasks. Other studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">54]</ref> delve into grounding smaller language models for control with domain-specific finetuning. Nevertheless, these methods often rely on manually designed controllers or code interfaces, sidestepping the challenge of learning low-level policies.</p><p>Plan4MC <ref type="bibr" target="#b48">[49]</ref> integrates LLM-based planning and RLbased policy learning but requires defining and pre-training all the policies with manually specified environments. Our RL-GPT extends LLMs' ability in low-level control by equipping it with RL, achieving automatic and efficient task learning in Minecraft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LLMs Agents</head><p>Several works leverage LLMs to generate subgoals for robot planning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Works like Inner Monologue <ref type="bibr" target="#b16">[17]</ref> incorporate environmental feedback into robot planning with LLMs. Code-as-Policies <ref type="bibr" target="#b20">[21]</ref> and ProgPrompt <ref type="bibr" target="#b31">[32]</ref> directly utilize LLMs to formulate executable robot policies. VIMA <ref type="bibr" target="#b17">[18]</ref> and PaLM-E <ref type="bibr" target="#b5">[6]</ref> involve fine-tuning pre-trained LLMs to support multimodal prompts. Besides, Chameleon <ref type="bibr" target="#b23">[24]</ref> effectively executes sub-task decomposition and generates sequential programs. ReAct <ref type="bibr" target="#b46">[47]</ref> utilizes chain-of-thought prompting to generate task-specific actions. AutoGPT <ref type="bibr" target="#b9">[10]</ref> automates NLP tasks by integrating reasoning and acting loops. DERA <ref type="bibr" target="#b25">[26]</ref> introduces dialogues between GPT-4 <ref type="bibr" target="#b0">[1]</ref> agents. Generative Agents <ref type="bibr" target="#b27">[28]</ref> simulate human behaviors by storing experiences as memories.</p><p>Compared with existing works, RL-GPT equips the LLM agent with RL, extending its capability in intricate low-level control in open-world tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Integrating LLMs and RL</head><p>Since LLMs and RL possess complementary abilities in providing prior knowledge and exploring unknown information, it is promising to integrate them for efficient task learning.</p><p>Most work studies improve RL with the domain knowledge in LLMs. SayCan <ref type="bibr" target="#b1">[2]</ref> and Plan4MC <ref type="bibr" target="#b48">[49]</ref> decompose and plan subtasks with LLMs, thereby RL can learn easier subtasks to solve the whole task. Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref> studies generating reward functions with LLMs to improve the sample efficiency for RL. Another line of research <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> finetunes LLMs with RL to acquire the lacked ability of LLMs in low-level control. However, these approaches usually require a lot of samples and can harm the LLMs' abilities in other tasks. Our study is the first to overcome the inabilities of LLMs in low-level control by equipping them with RL as a tool. The acquired knowledge is stored in context, thereby continually improving the LLMs skills and maintaining its capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>RL-GPT incorporates three distinct components, each contributing to its innovative design: (1) a slow agent tasked with decomposing a given task into several sub-actions and determining which actions can be directly coded, (2) a fast agent responsible for writing code and instantiating RL configuration, and (3) an iteration mechanism that facilitates an iterative process refining both the slow agent and the fast agent. This iterative process enhances the overall efficacy of the RL-GPT across successive iterations. For complex long-horizon tasks requiring multiple neural networks, we employ a GPT-4 as a planner to initially decompose the task.</p><p>As discussed in concurrent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, segregating high-level planning and low-level actions into distinct agents has proven to be beneficial. The dual-agent system effectively narrows down the specific task of each agent, enabling optimization for specific targets. Moreover, Liang et al. highlighted the Degeneration-of-Thought (DoT) problem, where an LLM becomes overly confident in its responses and lacks the ability for self-correction through self-reflection. Empirical evidence indicates that agents with different roles and perspectives can foster divergent thinking, mitigating the DoT problem. External feedback from other agents guides the LLM, making it less susceptible to DoT and promoting accurate reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RL Interface</head><p>As previously mentioned, we view the RL training pipeline as a tool accessible to LLMs agents, akin to other tools with callable interfaces. Summarizing the interfaces of an RL training pipeline, we identify the following components: 1) Learning task; 2) Environment reset; 3) Observation space; 4) Action space; 5) Reward function. Specifically, our focus lies on studying interfaces 1) and 4) to demonstrate the potential for integrating RL and Code-as-policy.</p><p>In the case of the action space interface, we enable LLMs to design high-level actions and integrate them into the action space. A dedicated token is allocated for this purpose, allowing the neural network to learn when to utilize this action based on observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Slow Agent: Action Planning</head><p>Assume the task T needs to be learned by a single network within constrained computing resources. We employ a GPT-4 <ref type="bibr" target="#b0">[1]</ref> as a slow agent A S . A S is tasked with decomposing T into sub-actions α i , where i ∈ {0, ..., n}, determining if each α i in T can be directly addressed through code implementation. This approach optimally allocates computational resources to address more challenging sub-tasks using RL. Importantly, A S is not required to perform any low-level coding tasks; it solely provides high-level textual instructions regarding sub-actions α i . These instructions are then transmitted to the fast agent A F for further processing. The iterative process of the slow agent involves systematically probing the limits of coding capabilities.</p><p>For instance, in Fig. <ref type="figure">3</ref>, consider the specific action of crafting a wooden pickaxe. Although A S is aware that players need to harvest a log, writing code for this task with a high success rate can be challenging. The limitation arises from the insufficient information available through APIs for A S to accurately locate and navigate to a tree. To overcome this hurdle, an RL implementation becomes necessary. RL aids A S in completing tasks by processing complex visual information and interacting with the environment through trial and error. In contrast, some simple, straightforward actions like crafting something with a crafting table can be directly coded and executed.</p><p>It is crucial to instruct A S to identify sub-actions that are too challenging for rule-based code implementation.</p><p>As shown in Table <ref type="table">1</ref>, the prompt for A S incorporates role description {role description}, the given task T , reference documents, environment knowledge {minecraft knowledge}, planning heuristics {planning tips}, and programming examples {programs}. To align A S with our goals, we include the heuristic in the {planning tips}. This heuristic encourages A S to further break down an action when coding proves challenging. This incremental segmentation aids A S in discerning what aspects can be coded. Further details are available in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fast Agent: Code-as-Policy and RL</head><p>The fast agent A F is also implemented using GPT-4. The primary task is to translate the instructions from the slow agent A S into Python codes for the sub-actions α i . A F undergoes a debug iteration where it runs the generated sub-action code and endeavors to self-correct through feedback from the environment. Sub-actions that can be addressed completely with code implementation are directly executed, as depicted in the blue segments of Fig. <ref type="figure">3</ref>. For challenging sub-actions lacking clear starting conditions, the code is integrated into the RL implementation using the temporal abstraction technique <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>, as illustrated in Fig. <ref type="figure">2</ref>. This involves inserting the high-level action into the RL action space, akin to the orange segments in Fig. <ref type="figure">3</ref>. A F iteratively corrects itself based on the feedback received from the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Two-loop Iteration</head><p>In Fig. <ref type="figure">4</ref>, we devise the two-loop iteration mechanism to optimize the proposed two agents, namely the fast agent A F and the slow agent A S . To facilitate it, a critic agent C is introduced, which could be implemented using GPT-3.5 or GPT-4.</p><p>The optimization for the fast agent, as shown in Fig. <ref type="figure">4</ref>, aligns with established methods for code-as-policy agents. Here, the fast agent receives a sub-action, environment documents D env (observation and action space), and examples E code as input, generating Python code. It then iteratively {role description} It is difficult to code all actions in this game. We only want to code as many sub-actions as possible. The task of you is to tell me which sub-actions can be coded by you with Python.</p><p>At each round of conversation, I will give you Task: T Context: ... Critique: The results of the generated codes in the last round Here are some actions coded by humans: {programs} You should then respond to me with Explain (if applicable): Why these actions can be coded by python? Are there any actions difficult to code? Actions can be coded: List all actions that can be coded by you.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Important Tips: {planning tips}</head><p>You should only respond in the format as described below: Explain: ... Actions can be coded: 1) Action1: ... 2) Action2: ... 3) ... Table <ref type="table">1</ref>. Slow Agent's prompt: Decompose a task into sub-actions.</p><p>refines the code based on environmental feedback. The objective is to produce error-free Python-coded sub-actions that align with the targets set by the slow agent. Feedback, which includes execution errors and critiques from C, plays a crucial role in this process. C evaluates the coded action's success by considering observations before and after the action's execution, offering insights for improvement.</p><p>Within Fig. <ref type="figure">4</ref>, the iteration of the slow agent A S encompasses the aforementioned fast agent A F iteration as a step. In each step of A S , A F must complete an iteration loop. Given a task T , D env , and E code , A S decomposes T into sub-actions α i and refines itself based on C's outputs. Specifically, it receives a sequence of outputs Critic i from C about each α i to assess the effectiveness of action planning. If certain actions cannot be coded by the fast agent, the slow agent adjusts the action planning accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Task Planner</head><p>Our primary pipeline is tailored for tasks that can be learned using a neural network within limited computational resources. However, for intricate tasks such as ObtainDiamond, where it is more effective to train multiple neural networks like DEPS <ref type="bibr" target="#b41">[42]</ref> and Plan4MC <ref type="bibr" target="#b48">[49]</ref>, we introduce At each round of conversation, I will give you Task: ... Context: ... Code from the last round: ... Execution error: ... Critique: ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You should then respond to me with Explain (if applicable): Can the code complete the given action? What does the chat log and execution error imply?</head><p>You should only respond in the format as described below: {code format} Table <ref type="table">2</ref>. Fast Agent's prompt: Write Python codes. a task planner reminiscent of DEPS, implemented using GPT-4. This task planner iteratively reasons what needs to be learned and organizes sub-tasks for our RL-GPT to accomplish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Environment</head><p>MineDojo MineDojo <ref type="bibr" target="#b6">[7]</ref> stands out as a pioneering framework developed within the renowned Minecraft game, tailored specifically for research involving embodied agents. This innovative framework comprises a simulation suite featuring thousands of tasks, blending both open-ended challenges and those prompted by language. To validate the effectiveness of our approach, we selected certain long-horizon tasks from MineDojo, mirroring the strategy employed in Plan4MC <ref type="bibr" target="#b48">[49]</ref>. These tasks include harvesting and crafting activities. For instance, Crafting one wooden pickaxe requires the agent to harvest a log, craft planks, craft sticks, craft tables, and craft the pickaxe with the table. Similarly, tasks like milking a cow involve the construction of a bucket, approaching the cow, and using the bucket to obtain milk. ObtainDiamond Challenge It represents a classic challenge for RL agents. The task of obtaining a diamond demands the agent to complete the comprehensive process of harvesting a diamond from the beginning. This constitutes a long-horizon task, involving actions such as harvesting logs, harvesting stones, crafting items, digging to find iron, smelting iron, locating a diamond, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>LLM Prompt We choose GPT-4 as our LLMs API. For the slow agents and fast agents, we design special templates, responding formats, and examples. We design some special prompts such as "assume you are an experienced RL researcher that is designing the RL training job for Minecraft". Details can be found in the Appendix A. In addition, we encourage the slow agent to explore more strategies because the RL task requires more exploring. We encourage the slow agent to further decompose the action into sub-actions which may be easier to code.</p><p>PPO Details Similar to MineAgent <ref type="bibr" target="#b6">[7]</ref>, we employ Proximal Policy Optimization (PPO) <ref type="bibr" target="#b29">[30]</ref> as the RL baseline. This approach alternates between sampling data through interactions with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent. PPO is constrained to a limited set of skills. When applying PPO with sparse rewards, specific tasks such as "milk a cow" and "shear a sheep" present challenges due to the small size of the target object relative to the scene, and the low probability of random encounters. To address this, we introduce basic dense rewards to enhance learning efficacy in these tasks. It includes the CLIP Reward, which encourages the agent to exhibit behaviors that align with the prompt <ref type="bibr" target="#b6">[7]</ref>. Additionally, we incorporate a Distance Reward that provides dense reward signals to reach the target items <ref type="bibr" target="#b48">[49]</ref>. Further details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>MineDojo Benchmark Table <ref type="table" target="#tab_2">3</ref> presents a comparative analysis between our RL-GPT and several baselines on selected MineDojo tasks. Notably, RL-GPT achieves the highest success rate among all baselines. All baselines underwent training with 10 million samples, and the checkpoint with the highest success rate was chosen for testing. MineAgent, as proposed in <ref type="bibr" target="#b6">[7]</ref>, combines PPO with Clip Reward. However, naive PPO encounters difficulties in learning long-horizon tasks, such as crafting a bucket and obtaining milk from a cow, resulting in an almost 0% success rate for MineAgent across all tasks. Another baseline, MineAgent with autocraft, as suggested in Plan4MC <ref type="bibr" target="#b48">[49]</ref>, incorporates crafting actions manually coded by humans. This alternative baseline achieves a 46% success rate on the milking task, demonstrating the importance of code-aspolicy. Our approach demonstrates superiority in coding actions beyond crafting, enabling us to achieve higher overall performance compared to baselines that focus primarily on crafting actions.</p><p>Plan4MC <ref type="bibr" target="#b48">[49]</ref> breaks down the problem into two essential components: acquiring fundamental skills and planning based on these skills. While some skills are acquired through Reinforcement Learning (RL), Plan4MC outperforms MineAgent due to its reliance on an oracle task decomposition from the GPT planner. However, it cannot modify the action space of an RL training pipeline or flexibly decompose sub-actions. It is restricted to only three types of human-designed coded actions. Consequently, our method holds a distinct advantage in this context. In tasks involving and , the agent is tasked with crafting a stick from scratch, necessitating the harvesting of a log. Our RL-GPT adeptly codes three actions for this: 1) Navigate to find a tree; 2) Attack 20 times; 3) Craft items. Notably, Action 2) can be seamlessly inserted into the action space. In contrast, Plan4MC is limited to coding craft actions only. This key distinction contributes to our method achieving higher scores in these tasks.</p><p>To arrive at the optimal code planning solution, RL-GPT undergoes a minimum of three iterations. As illustrated in Fig. <ref type="figure" target="#fig_2">5</ref>, in the initial iteration, RL-GPT attempts to code every action involved in harvesting a log, yielding a 0% success rate. After the first iteration, it decides to code navigation, aiming at the tree, and attacking 20 times. However, aiming at the tree proves too challenging for LLMs. As mentioned before, the agent will be instructed to further decompose the actions and give up difficult actions. By the third iteration, the agent correctly converges to the optimal solution-coding navigation and attacking, while leaving the rest to RL, resulting in higher performance.</p><p>In tasks involving crafting a wooden pickaxe and crafting a bed , in addition to the previously mentioned actions, the agent needs to utilize the crafting table. While Plan4MC must learn this process, our method can directly code actions to place the crafting table on the ground, use it, and recycle it. Code-as-policy contributes to our method achieving a higher success rate in these tasks.</p><p>In tasks involving crafting a furnace and a stone pickaxe , in addition to the previously mentioned actions, the agent is further required to harvest stones. Plan4MC needs to learn an RL network to acquire the skill of attacking stones. RL-GPT proposes two potential solutions for coding additional actions. First, it can code to continuously attack a stone and insert this action into the action space. Second, since LLMs understand that stones are underground, the agent might choose to dig deep for several levels to obtain stones instead of navigating on the ground to find stones.</p><p>In tasks involving crafting a milk bucket and crafting wool , the primary challenge lies in crafting a bucket or shears. Since both RL-GPT and Plan4MC can code actions to craft without a crafting table, their performance is similar and comparable.</p><p>In tasks involving obtaining beef and obtaining mutton , the only actions that can be further coded are navigating to find the target. Given that both RL-GPT and Plan4MC can code actions to navigate, their performance in these tasks is similar.</p><p>ObtainDiamond Challenge As shown in Tab. 4, we compare our method with existing competitive methods on the challenging ObtainDiamond task. DreamerV3 <ref type="bibr" target="#b12">[13]</ref> leverages a world model to accelerate exploration but still requires a significant number of interactions. Despite the considerable expense of over 100 million samples for learning, it only achieves a 2% success rate on the Diamond task from scratch.</p><p>VPT <ref type="bibr" target="#b2">[3]</ref> employs large-scale pre-training using YouTube videos to improve policy training efficiency. This strong baseline is trained on 80 GPUs for 6 days, achieving a 20% success rate in obtaining a diamond and a 2.5% success rate in crafting a diamond pickaxe.</p><p>DEPS <ref type="bibr" target="#b41">[42]</ref> suggests generating training data using a combination of GPT and human handcrafted code for planning and imitation learning. It attains a 0.6% success rate on this task. Moreover, an oracle version, which directly executes human-written codes, achieves a 60% success rate.</p><p>Plan4MC <ref type="bibr" target="#b48">[49]</ref> primarily focuses on crafting the stone pickaxe. Even with the inclusion of all human-designed actions from DEPS, it requires more than 7 million samples for training.</p><p>Our RL-GPT attains an over 8% success rate in the Ob-tainDiamond challenge by generating Python code and training a PPO RL neural network. Despite requiring some human-written code examples, our approach uses considerably fewer than DEPS. The final coded actions involve navigating on the ground, crafting items, digging to a specific level, and exploring the underground horizontally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We present ablation studies on our core designs in Tab. 5, Tab. 6, and Tab. 7, covering the framework structure, twoloop iteration, and RL interface.</p><p>Framework Structure In Tab. 5, we analyze the impact of the framework structure in RL-GPT, specifically examining different task assignments for various agents. Assigning all tasks to a single agent results in confusion due to the multitude of requirements, leading to a mere 0.34% success rate in crafting a table. Additionally, comparing the 3rd and 4th rows emphasizes the crucial role of a critic agent in our pipeline. Properly assigning tasks to the fast, slow, and critic agents can improve the performance to 0.65%. The slow agent faces difficulty in independently judging the suitability of actions based solely on environmental feedback and observation. Incorporating a critic agent facilitates more informed decision-making, especially when dealing with complex, context-dependent information.</p><p>Two-loop Iteration In Tab. 6, we ablate the importance of our two-loop iteration. Our iteration is to balance RL and code-as-policy to explore the bound of GPT's coding ability. We can see that pure RL and pure code-as-policy only achieve a low success rate on these chosen tasks. Our method can improve the results although there is no iteration (zero-shot). In these three iterations, it shows that the successful rate increases. It proves that the two-loop iteration is a reasonable optimization choice. Qualitative results can be found in Fig. <ref type="figure" target="#fig_2">5</ref>. Besides, we also compare the results with and without special prompts (SP) to encourage the LLMs to further decompose actions when facing coding difficulty. It shows that suitable prompts are also essential for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL Interface</head><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> explore the use of LLMs for RL reward design, presenting an alternative approach to combining RL and code-as-policy. With slight modifications, our fast agent can also generate code to design the reward function. However, as previously analyzed, reconstructing the action space proves more efficient than designing the reward function, assuming LLMs understand the necessary actions. Tab. 7 compares our method with the reward design approach, revealing that our method achieves a higher average success rate and lower dead loop ratio on our selected MineDojo tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In conclusion, we propose RL-GPT, a novel approach that integrates Large Language Models (LLMs) and Reinforcement Learning (RL) to empower LLMs agents on challenging tasks within complex, embodied environments. Our two-level hierarchical framework divides the task into highlevel coding and low-level RL-based actions, leveraging the strengths of both approaches. RL-GPT exhibits superior efficiency compared to traditional RL methods and existing GPT agents, achieving remarkable performance in challenging Minecraft tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes the task and determines "which actions" to learn. The fast agent writes code and RL configurations for low-level execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>{role description} Here are some basic actions coded by humans: {programs template} Please inherit the class CodeAgent. You are only required to overwrite the function main function. Here are some reference examples written by me: {programs example} Here are the attributes of the obs that can be used: {obs info} Here are the guidelines of the act variable: {act info}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution (RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration process. RL-GPT decomposes the task into "find a tree" and "cut a log", solving the former with code generation and the latter with RL. After a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success rate. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different methods on several tasks in the MineDojo benchmark. Our RL-GPT achieves the highest success rate on all tasks.</figDesc><table><row><cell>TASK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MINEAGENT</cell><cell>0.00 0.00 0.00 0.00 0.00 0.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">MINEAGENT (AUTOCRAFT) 0.00 0.03 0.00 0.00 0.00 0.46 0.50 0.33 0.35 0.00</cell></row><row><cell>PLAN4MC</cell><cell cols="5">0.30 0.30 0.53 0.37 0.17 0.83 0.53 0.43 0.33 0.17</cell></row><row><cell>RL-GPT</cell><cell cols="5">0.65 0.65 0.67 0.67 0.64 0.85 0.56 0.46 0.38 0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Main results in the challenging ObtainDiamond task in Minecraft. Existing strong baselines in ObtainDiamond either require expert data (VPT, DEPS), hand-crafted policies (DEPS-Oracle) for subtasks, or take huge number of environment steps to train (DreamerV3, VPT). Our method can automatically decompose and learn subtasks with only a little human prior, achieving ObtainDiamond with great sample efficiency.</figDesc><table><row><cell>METHOD</cell><cell>TYPE</cell><cell cols="2">SAMPLES SUCCESS</cell></row><row><cell>DREAMERV3</cell><cell>RL</cell><cell>100M</cell><cell>2%</cell></row><row><cell>VPT</cell><cell>IL+RL</cell><cell>16.8B</cell><cell>20%</cell></row><row><cell>DEPS-BC</cell><cell>IL+LLM</cell><cell>--</cell><cell>0.6%</cell></row><row><cell>DEPS-ORACLE</cell><cell>LLM</cell><cell>--</cell><cell>60%</cell></row><row><cell>PLAN4MC</cell><cell>RL+LLM</cell><cell>7M</cell><cell>0%</cell></row><row><cell>RL-GPT</cell><cell>RL+LLM</cell><cell>3M</cell><cell>8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the necessity of the proposed components in RL-GPT.</figDesc><table><row><cell>STRUCTURE</cell><cell></cell></row><row><cell>ONE AGENT</cell><cell>0.34 0.42</cell></row><row><cell>SLOW + FAST</cell><cell>0.52 0.56</cell></row><row><cell cols="2">SLOW + FAST + CRITIC 0.65 0.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on the effectiveness of our two-loop iteration strategy. RL-GPT achieves better results when the number of iterations increases.</figDesc><table><row><cell>METHOD</cell><cell></cell></row><row><cell>PURE RL</cell><cell>0.00 0.00 0.00 0.00</cell></row><row><cell>PURE CODE</cell><cell>0.13 0.02 0.00 0.00</cell></row><row><cell>OURS (ZERO-SHOT)</cell><cell>0.26 0.53 0.79 0.32</cell></row><row><cell cols="2">OURS (ITER-2 W/O SP) 0.26 0.53 0.79 0.30</cell></row><row><cell>OURS (ITER-2)</cell><cell>0.56 0.67 0.88 0.30</cell></row><row><cell>OURS (ITER-3)</cell><cell>0.65 0.67 0.93 0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on the RL interface: reward and action space design.</figDesc><table><row><cell>RL INTERFACE</cell><cell cols="2">SUCCESS RATE ↑ DEAD LOOP ↓</cell></row><row><cell>REWARD FUNCTION</cell><cell>0.418</cell><cell>≈0.6</cell></row><row><cell>ACTION SPACE</cell><cell>0.585</cell><cell>≈0.3</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Agent Prompt Details</head><p>Prompt details of fast and slow agent including {role description}, {planning tips},{act info}, and {obs info} are listed in <ref type="bibr">Table 8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10.</ref> {role description}: You are playing the game Minecraft. Assume you are a Python programmer. You want to write python code to complete some parts of this game.</p><p>{planning tips}: 1) If it is unsuccessful to code one action in the last round, it means the action is too difficult for coding.</p><p>2) If one action in the last round is too difficult to code, try to further subdivide the action. For example, if "attacking the tree 20 times" is difficult, try "simply attacking 20 times".</p><p>3) Please refer to the additional knowledge about Minecraft. It is very useful. We want to write python code to complete some actions in Minecraft. You are a helpful assistant that helps to write the code for the given action tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{act info}:</head><p>We design a compound action space. At each step the agent chooses one movement action (forward, backward, camera actions, etc.) and one optional functional action (attack, use, craft, etc.). Some functional actions such as craft take one argument, while others like attack does not take any argument. This compound action space can be modelled in an autoregressive manner.</p><p>Technically, our action space is a multi-discrete space containing eight dimensions: &gt;&gt;&gt; env.action space MultiDiscrete( <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">244,</ref><ref type="bibr" target="#b35">36]</ref>) Index 0; Forward and backward; 0: noop, 1: forward, 2: back Index 1; Move left and right; 0: noop, 1: move left, 2: move right Index 2; Jump, sneak, and sprint; 0: noop, 1: jump, 2: sneak, 3:sprint Index 3; Camera delta pitch; 0: -180 degree, 24: 180 degree Index 4; Camera delta yaw; 0: -180 degree, 24: 180 degree Index 5; Functional actions; 0: noop, 1: use, 2: drop, 3: attack, 4: craft, 5: equip, 6: place, 7: destroy Index 6; Argument for "craft"; All possible items to be crafted Index 7; Argument for "equip", "place", and "destroy"; Inventory slot indice </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details in PPO Implementations</head><p>CLIP reward. The reward incentivizes the agent to generate behaviors aligned with the task prompt. 31 task prompts are selected from the entire set of MineDojo programmatic tasks as negative samples. Utilizing the pre-trained MineCLIP model <ref type="bibr" target="#b6">[7]</ref>, we calculate the similarities between the features extracted from the past 16 frames and the prompts. The probability is then computed, indicating the likelihood that the frames exhibit the highest similarity to the given task prompt:</p><p>where f v , f l are video features and prompt features, l is the task prompt, and l − are negative prompts. The CLIP reward is:</p><p>Distance reward. The distance reward offers dense reward signals for reaching target items. In combat tasks, the agent receives a distance reward when the current distance is closer than the minimum distance observed in history:</p><p>For mining tasks involving or , where the agent needs to remain close to the block for several time steps, we adapt the distance reward to promote maintaining a small distance:</p><p>where d t is the distance between the agent and the target item at time step t, detected through lidar rays in the simulator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details in Minecraft Tasks</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. Gpt-4 technical report</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Michael Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<title level="m">Do as i can, not as i say: Grounding language in robotic affordances</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video pretraining (vpt): Learning to act by watching unlabeled online videos</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zhokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Sampedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="24639" to="24654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Carta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Romac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02662</idno>
		<title level="m">Grounding large language models in interactive environments with online reinforcement learning</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluating large language models trained on code</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Palm-e: An embodied multimodal language model</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03378</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building open-ended embodied agents with internet-scale knowledge</title>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>De-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Minedojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2022. 2, 5</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sipeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08922</idno>
		<title level="m">Llama rider: Spurring large language models to explore the open world</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03815</idno>
		<title level="m">Llm as os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous. github. retrieved</title>
		<author>
			<persName><surname>Gravitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-04-17">april 17, 2023, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A realworld webagent with planning, long context understanding, and program synthesis</title>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Safdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Faust</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.12856</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Brandon</forename><surname>William H Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholay</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13440</idno>
		<title level="m">Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mastering diverse domains through world models</title>
		<author>
			<persName><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgis</forename><surname>Pasukonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.04104</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00352</idno>
		<title level="m">Meta programming for multi-agent collaborative framework</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.08914</idno>
		<title level="m">A visual language model for gui agents</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9118" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inner monologue: Embodied reasoning through planning with language models</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05608</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>arXiv, 2022. 3</idno>
		<title level="m">Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Minerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions and Demonstrations Track</title>
		<author>
			<persName><forename type="first">Anssi</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolis</forename><surname>Ramanauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto mc-reward: Automated dense reward design with large language models for minecraft</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaokai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09238</idno>
		<imprint>
			<date type="published" when="2008">2023. 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Code as policies: Language model programs for embodied control</title>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9493" to="9500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Encouraging divergent thinking in large language models through multi-agent debate</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19118</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Steve-1: A generative model for text-tobehavior in minecraft</title>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00937</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09842</idno>
		<title level="m">Chameleon: Plug-and-play compositional reasoning with large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yecheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osbert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12931</idno>
		<title level="m">Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models</title>
				<imprint>
			<date type="published" when="2008">2023. 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Varun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Tso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17071</idno>
		<title level="m">Dera: enhancing large language model completions with dialog-enabled resolving agents</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling</title>
		<author>
			<persName><forename type="first">Kolby</forename><surname>Nottingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12050</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03442</idno>
		<title level="m">Generative agents: Interactive simulacra of human behavior</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accelerating reinforcement learning with learned skill priors</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngwoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<idno>PMLR, 2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
				<imprint>
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Building persona consistent dialogue agents with offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10735</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Progprompt: Generating situated robot task plans using large language models</title>
		<author>
			<persName><forename type="first">Ishika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11523" to="11530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Offline prompt evaluation and optimization with inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.06553</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Querydependent prompt evaluation and optimization with offline inverse rl</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alihan</forename><surname>Hüyük</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName><forename type="first">Doina</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Xagent: An autonomous agent for complex task solving</title>
		<author>
			<persName><forename type="first">Xagent</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Craft an iron sword: Dynamically generating interactive game characters by prompting large language models tuned on code</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Volum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Desgarennes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Wordplay: When Language Meets Games Workshop</title>
				<meeting>the 3rd Wordplay: When Language Meets Games Workshop</meeting>
		<imprint>
			<date type="published" when="2022">Wordplay 2022. 2022</date>
			<biblScope unit="page" from="25" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Voyager: An open-ended embodied agent with large language models</title>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16291</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16090</idno>
		<title level="m">Tpe: Towards better compositional reasoning over conceptual tools with multi-persona collaboration</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.11432</idno>
		<title level="m">A survey on large language model based autonomous agents</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05997</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01560</idno>
		<imprint>
			<date type="published" when="2008">2023. 1, 2, 5, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Text2reward: Automated dense reward function generation for reinforcement learning</title>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11489</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Chengrun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.03409</idno>
		<title level="m">Large language models as optimizers</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Octopus: Embodied vision-language programmer from environmental feedback</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chencheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08588</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zebiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.13771</idno>
		<title level="m">Appagent: Multimodal agents as smartphone users</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">React: Synergizing reasoning and acting in language models</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Language to rewards for robotic skill synthesis</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Gileadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Gonzalez Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Hao-Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hasenclever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.08647</idno>
		<imprint>
			<date type="published" when="2023-01">Jan Humplik,. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks</title>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penglin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16563</idno>
		<imprint>
			<date type="published" when="2008">2023. 3, 5, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pre-training goal-based models for sample-efficient reinforcement learning</title>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhancun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Creative agents: Empowering agents with imagination for creative tasks</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penglin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02519</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Large language model is semiparametric reinforcement learning agent</title>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Situo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07929</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Wanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17176</idno>
		<title level="m">Rladapter: Bridging large language models to reinforcement learning in open worlds</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds</title>
		<author>
			<persName><forename type="first">Sipeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13255</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">RGB frames provide an egocentric view of the running Minecraft client that is the same as human players see. Data type: numpy.uint8 Shape: (3, H, W), height and width are specified by argument image size obs</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17144,2023.2obs[&quot;rgb&quot;]:</idno>
	</analytic>
	<monogr>
		<title level="m">Names of inventory items in natural language</title>
				<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. such as &quot;obsidian&quot; and &quot;cooked beef&quot;. Data type: str Shape</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">This type of observation is similar to how human players perceive their surrounding blocks. It includes names and properties of blocks. obs</title>
	</analytic>
	<monogr>
		<title level="m">Names of surrounding blocks in natural language</title>
				<imprint/>
	</monogr>
	<note>We also provide voxels observation (3x3x3 surrounding blocks around the agent). such as &quot;dirt&quot;, &quot;air&quot;, and &quot;water&quot;. Data type: str Shape: (3, 3, 3) obs[&quot;location stats&quot;][&quot;pos&quot;]: The xyz position of the agent. Data type: numpy.float32 Shape: (3,) obs[&quot;location stats&quot;][&quot;yaw&quot;] and obs[&quot;location stats&quot;][&quot;pitch&quot;]: Yaw and pitch of the agent. Data type: numpy.float32 Shape: (1,) obs[&quot;location stats&quot;][&quot;biome id</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">It includes three parts: information about traced entities, properties of traced blocks, and directions of lidar rays themselves. obs</title>
	</analytic>
	<monogr>
		<title level="m">Distances to traced entities. Data type: numpy.float32 Shape: (num rays</title>
				<imprint/>
	</monogr>
	<note>Properties of traced blocks include blocks&apos; names and distances from the agent. obs. block name&quot;]: Names of traced blocks in natural language in the fan-shaped area ahead of the agent, such as &quot;dirt&quot;, &quot;air&quot;, and &quot;water&quot;. Data type: str Shape: (num rays,) obs[&quot;rays&quot;][&quot;block distance&quot;]: Distances to traced blocks in the fan-shaped area ahead of the agent. Data type: numpy.float32 Shape: (num rays,) Table 10. Observation information {obs info} of Fast Agent</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
