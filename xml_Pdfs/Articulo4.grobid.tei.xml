<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-02-29">29 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
							<email>yifei_zhou@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Zanette</surname></persName>
							<email>zanette@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
							<email>aviralkumar@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-29">29 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">D1C32795F7AE9CBAF9D0190D4540B926</idno>
					<idno type="arXiv">arXiv:2402.19446v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-03T17:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate probable completions for a given prompt, but rather make intelligent decisions over an extended period of multi-turn interaction to accomplish a task (e.g., when interacting with the web, using software tools, or engaging in customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on single-turn reward maximization. By construction, single-turn RL methods of today cannot actually train LLMs to intelligently seek and incorporate information over multiple turns, perform credit assignment, or reason about their past actions -all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we propose an algorithmic framework for developing multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function (in place of a reward model used in single-turn RL) to train a token-by-token policy within each utterance or turn. This hierarchical approach prescribed by our framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to a number of other RL approaches. Empirically, we find that ArCHer significantly improves efficiency and performance on multi-turn tasks, attaining sample efficiency of about 100x over existing on-policy methods, while also benefitting favorably from scaling up model capacity (upto the 7 billion scale that we could test on in our experiments). Project page can be found in https://yifeizhou02.github.io/archer.io/ and code can be found in https://github.com/YifeiZhou02/ArCHer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Owing to their generalist knowledge, large language models (LLMs) have a tremendous potential to address a wide variety of decision-making or "agent" problems that can be expressed in text or natural language, from writing code <ref type="bibr" target="#b52">(Yang et al., 2023a;</ref><ref type="bibr" target="#b22">Li et al., 2022;</ref><ref type="bibr" target="#b23">Lin et al., 2018)</ref>, navigating the web <ref type="bibr" target="#b60">(Zhou et al., 2023a;</ref><ref type="bibr" target="#b54">Yao et al., 2023a)</ref>, and using tools <ref type="bibr" target="#b37">(Schick et al., 2023)</ref>, all the way to interacting with humans <ref type="bibr" target="#b10">(Ghosal et al., 2022;</ref><ref type="bibr" target="#b47">Verma et al., 2022;</ref><ref type="bibr" target="#b17">Jaques et al., 2020)</ref>. In order to succeed in these domains, an LLM needs to make a sequence of intelligent decisions over multiple turns of interaction instead of generating the most probable text completion at each step. Despite these multi-turn agent problems, most methods for eliciting goal-directed behavior from LLMs often rely on myopic objectives that either attempt to mimic successful demonstrations at each step <ref type="bibr" target="#b58">(Zeng et al., 2023;</ref><ref type="bibr" target="#b4">Chen et al., 2023)</ref>, or otherwise optimize for single-turn preferences <ref type="bibr" target="#b45">(Touvron et al., 2023;</ref><ref type="bibr" target="#b29">Ouyang et al., 2022;</ref><ref type="bibr" target="#b1">Bai et al., 2022)</ref>. Policies trained via single-turn approaches often fail to perform effective credit assignment (e.g., they fail to identify good actions that may lead to long-term future performance despite appearing suboptimal at a given step) and do not endow policies with information-seeking behavior, which is important in agent problems (e.g., when dealing with a new tool). Therefore, in this paper, we consider the problem of building multi-turn RL approaches that are able to directly maximize long-term objective of interest (e.g., customer satisfaction at the end of a multi-turn conversation with an LLM assistant), formulated via a scalar reward function.  In particular, the single-turn RL agent seeks to resolve the request within a single turn and hence ends up providing as much information as possible in its response. On the contrary, the multi-turn RL agent can execute information-gathering actions and address requests in a targeted manner over turns. While current single-turn RL methods for LLMs abstractly use some form of policy gradients computed using a reward model to train the LLM policy, the method proposed in our paper extends this paradigm to the multi-turn setting by now replacing the reward model with a learned value function, which is trained with off-policy reinforcement learning.</p><p>Unlike single-step RL <ref type="bibr" target="#b45">(Touvron et al., 2023;</ref><ref type="bibr" target="#b29">Ouyang et al., 2022;</ref><ref type="bibr" target="#b1">Bai et al., 2022)</ref>, training LLMs via multi-turn RL presents a number of unique challenges. First of all, multi-turn RL would require online interaction with external sources such as humans or web servers, which can be slow and expensive. Due to this, on-policy methods such as PPO <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref> quickly become impractical owing to their inability to reuse data from past interaction. While off-policy fully offline methods circumvent this problem, these methods present other challenges: since the number of tokens increase with multiple turns (often substantially so, due to the bias of LLMs towards producing long responses), token-level methods <ref type="bibr" target="#b41">(Snell et al., 2023;</ref><ref type="bibr" target="#b17">Jaques et al., 2020)</ref> that consider individual tokens as actions need to now propagate reward signal over extremely long horizons. This results in extremely slow learning speeds over long horizons for token-level algorithms. For example, token-level ILQL <ref type="bibr" target="#b41">(Snell et al., 2023)</ref> takes more than 10 days to converge on a task in our experiments, while filtered behavior cloning takes less than a day. In principle, to address long horizon issues, one can treat the entire utterance for each turn as an action <ref type="bibr" target="#b47">(Verma et al., 2022;</ref><ref type="bibr" target="#b16">Jang et al., 2022)</ref>, but this comes at the cost of introducing an enormous, variable-size action space, presenting a challenge for off-policy methods based on temporal-difference (TD) learning that require maximization over the action at each time step. This necessitates a multi-turn RL framework that can attain a sweet spot in terms of the aforementioned challenges.</p><p>In this paper, we devise a framework for building multi-turn RL algorithms that attains this kind of a sweet spot. Our key insight is that a hierarchical approach for RL with language models that addresses the challenges with both on-policy and off-policy RL approaches as outlined above. Specifically, our framework prescribes an off-policy temporal difference learning method for training an utterance-level value function at the high level, and any on-policy policy gradient algorithm for optimizing the token generation at each turn of the interaction at the low level, treating the high-level value function as the terminal reward for that turn. Unlike on-policy methods, this allows for sample reuse and faster convergence, while avoiding Bellman backups over individual tokens or maximization over enormous action spaces, as the high-level critic is trained at a coarser time-scale, on tokens produced by the actor. In addition, it also directly inherits implementation details from existing token-level RL algorithms developed for single-turn RL with preferences, for training the policy. This way we are able to obtain the best of both utterance-based and token-based, and off-policy and on-policy approaches for training LLMs.</p><p>Our main contribution is a framework for developing hierarchical RL approaches for LLMs, that we call: Actor-Critic framework with a Hierarchical Structure (or ArCHer in short). We study several concrete algorithmic instantations derived from the ArCHer framework by conducting experiments on a range of language "agent" tasks with active data collection (i.e., the "online" setting). We find that algorithms derived from ArCHer are 100x more sample efficient than on-policy methods such as PPO, and converge to a better performance than off-policy methods. Moreover, our methods are easy to build on existing single-turn RL methods and scale to different transformer architectures and more parameters (we show effectiveness of our approach up to the 7 billion scale), directly enabling plug-and-play choices of RL algorithms and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-turn reinforcement learning for LLMs. Most prior works that use RL for LLMs have focused on decision-making problems where the language model must produce a single decision, with no further steps of interaction with an external environment (e.g., the "single-turn" preference optimization setting <ref type="bibr" target="#b3">(Casper et al., 2023;</ref><ref type="bibr" target="#b5">Christiano et al., 2023;</ref><ref type="bibr" target="#b63">Ziegler et al., 2019)</ref>). Typical algorithms used for this sort of single-turn RL include policy-gradient methods such as PPO <ref type="bibr" target="#b29">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b28">OpenAI et al., 2023;</ref><ref type="bibr" target="#b35">Ramamurthy et al., 2023)</ref>, A2C <ref type="bibr" target="#b11">(Glaese et al., 2022)</ref>, offline optimization methods such as DPO <ref type="bibr" target="#b33">(Rafailov et al., 2023)</ref>, and filtered supervised learning approaches <ref type="bibr" target="#b56">(Yuan et al., 2023;</ref><ref type="bibr" target="#b19">Korbak et al., 2023;</ref><ref type="bibr" target="#b12">Gulcehre et al., 2023)</ref>. Despite promising results, there remain many important agent problems that cannot be solved in a single-turn setting. Many of these problems require the agent to explicitly take the steps to gather information before making a decision, such as asking for personalized preferences before making a travel plan <ref type="bibr" target="#b15">(Hong et al., 2023)</ref> or initially attempting to read the help manual of the shell in a linux terminal, before carrying out the requested tasks <ref type="bibr" target="#b24">(Liu et al., 2023)</ref>. Single-step approaches cannot learn such nuanced strategies as they attempt to solve the problem within a single step, necessitating multi-turn RL methods for training LLMs.</p><p>Training language agents without RL. Motivated by few-shot learning and reasoning abilities of LLMs, prior works also utilize LLMs for sequential decision-making via prompt engineering. For example, ReAct <ref type="bibr" target="#b55">(Yao et al., 2023b)</ref> and Reflexion <ref type="bibr" target="#b40">(Shinn et al., 2023)</ref> prompt the LLM to "think" and analyze past failures before executing the next action. Voyager <ref type="bibr" target="#b48">(Wang et al., 2023)</ref> prompts the LLM agent to develop and refine a curriculum and action library based on environment input. However, without updating the parameters of the LLM, the effectiveness of these methods is inherently limited by the intrinsic capabilities obtained from pre-training <ref type="bibr" target="#b58">(Zeng et al., 2023;</ref><ref type="bibr" target="#b4">Chen et al., 2023)</ref>. Even state-of-the-art models such as GPT-4 with in-context learning can perform very sub-optimally in out-of-distribution settings <ref type="bibr" target="#b24">(Liu et al., 2023;</ref><ref type="bibr" target="#b53">Yang et al., 2023b)</ref> without updating the model. To improve over pre-trained capabilities, another line of work finetunes LLMs with successful trajectories (generated manually or by rolling out a strong pre-trained LLM) <ref type="bibr" target="#b37">(Schick et al., 2023;</ref><ref type="bibr" target="#b58">Zeng et al., 2023;</ref><ref type="bibr" target="#b4">Chen et al., 2023)</ref>. However, manual labels and tool call annotations are be expensive to obtain. Moreover, it would be prohibitively expensive for automated approaches to stumble upon successful rollouts will as the task horizon increases <ref type="bibr" target="#b24">(Liu et al., 2023;</ref><ref type="bibr" target="#b0">Abdulhai et al., 2023)</ref>. Therefore, in this paper, we sidestep these problems by directly maximizing the objective of interest via RL. Multi-turn RL for LLMs. While many prior works directly use off-the-shelf policy-gradient methods, such as PPO <ref type="bibr" target="#b39">(Schulman et al., 2017;</ref><ref type="bibr" target="#b44">Szot et al., 2023;</ref><ref type="bibr" target="#b54">Yao et al., 2023a) and</ref><ref type="bibr">REINFORCE (Sutton et al., 1999;</ref><ref type="bibr" target="#b49">Williams, 2004;</ref><ref type="bibr" target="#b36">Ranzato et al., 2015;</ref><ref type="bibr" target="#b50">Wu and Hu, 2018;</ref><ref type="bibr" target="#b30">Paulus et al., 2017)</ref> to train LMs, these methods can become sample inefficient in multi-step settings that require interaction with an external environment <ref type="bibr" target="#b47">(Verma et al., 2022;</ref><ref type="bibr" target="#b16">Jang et al., 2022)</ref>. To address such sample complexity issues, off-policy and offline value-based methods learn from existing static data <ref type="bibr" target="#b41">(Snell et al., 2023;</ref><ref type="bibr" target="#b17">Jaques et al., 2020;</ref><ref type="bibr" target="#b47">Verma et al., 2022;</ref><ref type="bibr" target="#b16">Jang et al., 2022)</ref>. However, existing off-policy methods for multi-turn language tasks either (1) consider a single token as an action (i.e., "token-level") <ref type="bibr" target="#b41">(Snell et al., 2023</ref>;  Our algorithm operates both at the utterance level and the token level. At the utterance level, our algorithm learns a Q-function, via Bellman bootstrapping with TD errors. At the token level, the policy is learned by maximizing the advantage function induced by the utterance-level Q-function using a policy gradient approach, where this advantage estimate is provided as a reward at the end of the sequence of tokens appearing within the utterance. <ref type="bibr" target="#b17">Jaques et al., 2020)</ref> and must deal with long horizons, or (2) consider an utterance as a single action, but utilize multiple candidate utterances from a frozen pre-trained LLM for maximization in the Bellman backup <ref type="bibr" target="#b47">(Verma et al., 2022;</ref><ref type="bibr" target="#b16">Jang et al., 2022)</ref>, reducing the pace of policy improvement, as we also find in our experiments. Our approach will address both of these limitations.</p><formula xml:id="formula_0">s Utterance-Level Critic CLS a s′ V π (s′ ) Utterance-Level Critic CLS Q π (s, a) ← r + γV π (s′ )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Actor-Critic Framework with a Hierarchical Structure (ArCHer)</head><p>Existing RL methods that consider an individual token as an action suffer from very long horizons with multiple turns. Utterance-level RL methods avoid this challenge, but now they must tractably maximize over a coherent set of tokens within an utterance. To address these issues, we will develop a class of hierarchical RL methods, ArCHer, that bypass these challenges by running two RL algorithms, in parallel. We start by describing how multi-turn language generation can be posed as a hierarchical Markov decision process (MDP), followed by building RL methods in this hierarchical MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Language Generation as a Hierarchical MDP</head><p>In order to derive effective multi-turn RL algorithms, we will now present a novel formulation of language generation as acting in a hierarchical MDP. Our construction defines a high-level MDP, and a low-level MDP embedded inside the high-level MDP. For both MDPs, states are a variable-length sequence of tokens. For the high-level MDP, an action is defined as a sequence of tokens. An action in the low-level MDP is a single token, such that executing a sequence of actions in the low-level MDP corresponds to a single action in the high-level MDP. Formally, each state 𝑠 𝑡 in the high-level MDP consists of an interaction history between the LLM and the external environment, and each action 𝑎 𝑡 in this MDP is a variable-length sequence of tokens. The low-level MDP models the generation of the high-level action produced by the agent within a single high-level action, where each low-level action 𝑎 ℎ 𝑡 is an individual token (i.e., the ℎ-th token in the 𝑡-th high-level action). A state in this low-level MDP is obtained by concatenating a high-level state 𝑠 𝑐 consisting of the interaction history until this turn and 𝑎 1:ℎ−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑡</head><p>, the history of individual action tokens produced within the current turn before step ℎ. The next state is obtained by concatenating the current action to this token history. Figure <ref type="figure" target="#fig_2">2</ref> shows a visualization of our notations for the hierarchical MDP.</p><p>Policy optimization in the high-level MDP aims to maximize task reward, whereas a policy in the low-level MDP attempts to find a sequence of up to 𝐿 tokens 𝑎 1:𝐿 𝑡 that maximizes reward equal to the value function of the high-level MDP 𝑄 𝜋 (𝑠, 𝑎 1:𝐿 𝑡 ), provided at the end of the low-level rollout. A rollout in the low-level MDP ends as soon as the policy ends up choosing to produce an "EOS" token.</p><p>A concrete and natural instantiation of this hierarchical framework in the context of multi-turn interactions is when each turn or an "utterance" corresponds to a single time step in the high-level MDP, and each token within a turn is an action in the low-level MDP. In other words, this construction chooses to use the utterance-level MDP (Figure <ref type="figure" target="#fig_2">2</ref>) at the high level and the token-level MDP at the low level. For example, in the context of a web agent, the state would be the interaction of web pages visited so far and a high-level action would be an utterance, e.g., "search[queen-sized bed, black]". A candidate reward function would be +1 if the correct item can be bought and 0 otherwise. The dynamics would involve the web engine reacting to an action. Each token of an utterance (e.g., "search[queen-sized bed, black]") would be an individual action in the embedded token-level MDP, for example, an action would be individual tokens "search", "[", "queen".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminaries: Reinforcement Learning Definitions</head><p>In order to describe the details of our framework, we first provide a few standard RL definitions. The Q function of a policy 𝜋 is the expected long-term return obtained by executing a certain action at the current step, followed by executing 𝜋 thereafter:</p><formula xml:id="formula_1">𝑄 𝜋 (𝑠 ℎ , 𝑎 ℎ ) = E 𝜋 [︀∑︀ ∞ 𝑡=0 𝛾 𝑖 𝑟(𝑠 ℎ+𝑡 , 𝑎 ℎ+𝑡 )</formula><p>]︀ . The value function 𝑉 𝜋 (𝑠 ℎ ) is given by taking an expectation of the Q-value, 𝑄 𝜋 (𝑠 ℎ , 𝑎 ℎ ), under actions 𝑎 ℎ sampled from the policy 𝜋. The advantage 𝐴 𝜋 (𝑠 ℎ , 𝑎 ℎ ) of a state-action pair is the difference between its Q-value and the value of the state under the policy: 𝐴 𝜋 (𝑠 ℎ , 𝑎 ℎ ) = 𝑄 𝜋 (𝑠 ℎ , 𝑎 ℎ ) − 𝑉 𝜋 (𝑠 ℎ ). We will denote the value function in the low-level MDP as ̃︀ 𝑉 (𝑠 𝑐 , 𝑎 1:𝑖−1 ℎ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RL Algorithms in the Hierarchical Language MDP</head><p>The proposed hierarchical MDP provides flexibility in designing multi-turn RL algorithms: we could use any choice of RL algorithm for either the high or the low level. That said, note that only the high level requires interaction with a (non-differentiable) environment, while the low level optimizes against the high-level value function, and therefore trains entirely "in silico," without any interaction with an environment. Therefore, the requirements on these methods are different: the high-level algorithm should be highly sample efficient, while the low-level algorithm should be easy to optimize. A particularly convenient choice is to use TD learning at the high level, while using on-policy methods <ref type="bibr" target="#b29">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b1">Bai et al., 2022)</ref> at the low level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A Practical Instantiation of ArCHer for Sample-Efficient Online RL</head><p>For deriving a concrete practical algorithm, we will utilize the natural hierarchical MDP induced in multi-turn language interaction: the utterance-level MDP at the high level and the embedded token-level MDP at low level, as discussed at the end of Section 3.1. In this setting, our approach would train an utterance-level critic with TD backups and a token-level policy with policy gradients.</p><p>High-level utterance critic. Following the practice in prior RL algorithms <ref type="bibr" target="#b41">(Snell et al., 2023)</ref>, we train two LLM models at the high-level, one to represent the utterance-level Q-function 𝑄 𝜋 𝜃 (𝑠, 𝑎), and one to represent the utterance-level value function, 𝑉 𝜋 𝜓 (𝑠). The Q-model is trained on Bellman targets computed from a delayed copy of the value-model. And the value model, in turn, is trained to approximate the expected value of the Q-model on token sequences (i.e., utterances) obtained by sampling autoregressively from the low-level policy, 𝜋 𝜑 . Due to the off-policy nature of this training process, we store and train on data from all previous online interactions 𝒟 = {𝑠 𝑖 , 𝑎 𝑖 , 𝑟 𝑖 , 𝑠 ′ 𝑖 } 𝑁 𝑖=1 . The objective for training the Q-model and the value-model are formally given by:</p><formula xml:id="formula_2">𝐽 𝑄 (𝜃) = E 𝑠,𝑎,𝑟,𝑠 ′ ∼𝒟 [︀ (𝑄 𝜃 (𝑠, 𝑎) − 𝑟 − 𝛾𝑉 ψ(𝑠 ′ )) 2 ]︀</formula><p>. (Bellman consistency between 𝑄 𝜃 and 𝑉 ψ) (1)</p><formula xml:id="formula_3">𝐽 𝑉 (𝜓) = E 𝑠∼𝒟 [︁ E 𝑎∼𝜋 𝜑 (•|𝑠) [︀ (𝑉 𝜓 (𝑠) − 𝑄θ(𝑠, 𝑎)) 2 ]︀ ]︁ . (Train 𝑉 𝜓 to approximate E 𝑎∼𝜋 𝜑 (•|𝑠) [𝑄θ(𝑠, 𝑎)])<label>(2)</label></formula><p>We estimate Equation 2 by sampling a batch of 𝑛 observations {𝑠 𝑖 } 𝑛 𝑖=1 , followed by auto-regressively sampling token sequences from the actor {𝑎 𝐿 𝑖 } 𝑛 𝑖=1 . The delayed target models 𝑄θ and 𝑉 ψ are updated towards their current counterparts with Polyak averaging <ref type="bibr" target="#b13">(Haarnoja et al., 2018)</ref>.</p><p>Low-level token actor. At the low-level, we train the token-level actor 𝜋 𝜑 (•|𝑠 𝑐 , 𝑎 1:ℎ 𝑡 ) via an on-policy policy gradient approach to find a sequence of tokens that maximizes the prediction of the Q-model. To reduce variance, we use advantage values derived from the Q-model as the terminal reward. This subroutine for training the actor generalizes single-turn RL methods from RLHF, except that the terminal reward is now an estimate of the multi-turn advantage instead of a reward model. Concretely, we update the token-level policy with the policy gradient computed via REINFORCE <ref type="bibr" target="#b49">(Williams, 2004)</ref>:</p><formula xml:id="formula_4">𝐽 𝜑 (𝜋) = E 𝑠𝑐∼𝒟,𝑎 1:𝐿 𝑡 ∼𝜋(•|𝑠𝑐) [︃ 𝐿 ∑︁ 𝑖=1 𝐴(𝑠 𝑐 , 𝑎 1:𝐿 𝑡 ) log 𝜋 𝜑 (𝑎 𝑖 𝑡 |𝑠 𝑐 , 𝑎 1:𝑖−1 𝑡 ) ]︃ .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Other Practical ArCHer Algorithms</head><p>The instantiation of ArCHer described in Section 3.4 is simple and ready-to-use, but the flexibility of the ArCHer framework also enables it to incorporate other components that prior works found to be successful. We describe two variants in the two paragraphs to follow. The first improves the sample efficiency while interacting with the external environment and the second enables ArCHer to learn entirely from a dataset of pre-collected experience.</p><p>Improvements to token-level policy gradient. In applications where the horizon of the token-level MDP is long (i.e. each utterance has many tokens), despite the use of advantage values (instead of Q-model predictions directly), the REINFORCE estimator corresponding to Equation 3 can struggle to improve the policy due to a high variance in token-level reward <ref type="bibr" target="#b38">(Schulman et al., 2015)</ref>. This variance can be reduced by introducing a baseline value function, ̃︀ 𝑉 𝜂 (̃︀ 𝜋) parameterized by 𝜂, in the token-level MDP. For simplicity, we opt to train this token-level baseline via supervised regression onto Monte-Carlo return estimates (with a discount factor of 1.0) in the token-level MDP as shown in Equation <ref type="formula" target="#formula_5">4</ref>, though Bellman backups can also be employed in the low-level MDP to estimate it:</p><formula xml:id="formula_5">𝐽 𝜂 ( ̃︀ 𝑉 ) = E 𝑠𝑐∼𝒟,𝑎 1:𝐿 𝑡 ∼𝜋(•|𝑠𝑐) [︃ 𝐿 ∑︁ 𝑖=1 (︁ 𝐴(𝑠 𝑐 , 𝑎 1:𝐿 𝑡 ) − ̃︀ 𝑉 𝜂 (𝑠 𝑐 , 𝑎 1:𝑖−1 𝑡 ) )︁ 2 ]︃ . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Incorporating this token-level baseline, the new objective for the actor given by Equation <ref type="formula" target="#formula_7">5</ref>:</p><formula xml:id="formula_7">𝐽 𝜑 (𝜋) = E 𝑠𝑐∼𝒟,𝑎 1:𝐿 𝑡 ∼𝜋(•|𝑠𝑐) [︃ 𝐿 ∑︁ 𝑖=1 (︁ 𝐴(𝑠 𝑐 , 𝑎 1:𝐿 𝑡 ) − ̃︀ 𝑉 𝜂 (𝑠 𝑐 , 𝑎 1:𝑖−1 𝑡 ) )︁ • log 𝜋 𝜑 (𝑎 𝑖 𝑡 |𝑠 𝑐 , 𝑎 1:𝑖−1 𝑡 ) ]︃ . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Offline RL training with ArCHer. ArCHer can also learn from a dataset of pre-collected experience without any online interaction. A distinguishing aspect of the offline setting is that improving the policy normally results in selecting out-of-distribution actions <ref type="bibr" target="#b21">(Kumar et al., 2019)</ref>, whose 𝑄 values are difficult to estimate accurately given only the offline dataset. As a result, directly optimizing the 𝑄 values, as in the online setting, often results in severe overestimation, divergence, and poor policy performance. This suggests that we need to adopt different objective functions in this offline setting. One concrete instantiation is to utilize the implicit Q-learning (IQL) <ref type="bibr" target="#b20">(Kostrikov et al., 2021)</ref> algorithm for obtaining backup targets for the utterance-level critic restricted to in-support actions, and the AWR <ref type="bibr" target="#b31">(Peng et al., 2019)</ref> algorithm for imposing a penalty on deviating far away from the data on the actor. While our offline RL experiments utilize these design choices, one could also utilize other techniques such as explicitly regularizing the critic's predictions <ref type="bibr" target="#b21">(Kumar et al., 2020)</ref> or imposing a behavioral cloning loss on the actor <ref type="bibr" target="#b8">(Fujimoto and Gu, 2021)</ref>.</p><p>The IQL loss aims to derive a version of the TD error that aims to inherit characteristics of the Bellman optimality operator but without performing an explicit maximization over the actions, by instead regressing Q-functions towards a higher expectile of possible target values at the next state. For a given expectile parameter 𝜏 ∈ [0.5, 1), the IQL loss is given by the following:</p><formula xml:id="formula_9">𝐽 IQL 𝜓 (𝑉 ) = E 𝑠∼𝒟 [E 𝑎∼𝜋 𝜑 (•|𝑠) [𝐿 𝜏 2 (𝑉 𝜓 (𝑠) − 𝑄θ(𝑠, 𝑎))]],<label>(6)</label></formula><p>where <ref type="bibr" target="#b20">Kostrikov et al. (2021)</ref> for more details. The policy extracted by AWR <ref type="bibr" target="#b31">(Peng et al., 2019)</ref> trades off between searching for high-return policies and imitating the policy that generated the dataset, by minimizing the loss</p><formula xml:id="formula_10">𝐿 𝜏 2 (𝑢) = |𝜏 − 1{𝑢 &lt; 0}|𝑢 2 . See the paper</formula><formula xml:id="formula_11">𝐽 𝜑 (𝜋) = −E (𝑠𝑐,𝑎 1:𝐿 𝑡 )∼𝒟 [︃ exp (︀ 𝛽 • 𝐴(𝑠 𝑐 , 𝑎 1:𝐿 𝑡 ) )︀ • 𝐿 ∑︁ 𝑖=1 log 𝜋 𝜑 (𝑎 𝑖 𝑡 |𝑠 𝑐 , 𝑎 1:𝑖−1 𝑡 ) ]︃ . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>The tradeoff is controlled by a positive, user-defined scalar value 𝛽. Low values for 𝛽 encourage imitating the policy that generated the dataset. Large values of 𝛽 lead to more aggressive maximization of rewards, but potentially at the cost of stability <ref type="bibr" target="#b31">(Peng et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Framework Summary and Practical Implementation Details</head><p>Pseudocode. The algorithms derived from the ArCHer framework so far are summarized in Algorithm 1. These algorithms can operate in either offline or online mode (Line 4), and can utilize a variety of objectives for training the utterance-level Q-and V-models (Lines 9-15) as well the token-level policy (Line 20). Optionally, a token-level baseline value function may also be utilized (Line 17).</p><p>Implementation details. In our main experiments, we use a GPT-2 <ref type="bibr" target="#b32">(Radford et al., 2019)</ref> architecture for parameterizing the policy, and a RoBERTa-base model <ref type="bibr" target="#b25">(Liu et al., 2019)</ref> with a linear layer on top of the embeddings corresponding to the "[CLS]" token for obtaining the critic's predictions. To address the issue of overestimation of Q-values, we also employ the double Q-learning trick <ref type="bibr" target="#b46">(van Hasselt et al., 2015)</ref> and train two copies of Q-and V-models, {𝑄 1 , 𝑉 1 } and {𝑄 2 , 𝑉 2 }, independently.</p><p>The advantage value is calculated by using a minimum over 𝑄 1 , 𝑄 2 and 𝑉 1 , 𝑉 2 .</p><p>To save computation and memory costs, 𝑄 1 , 𝑄 2 , 𝑉 1 , 𝑉 2 share the same language model encoder backbone with separate MLP heads. The parameters of the token-level actor are independent from the critic. When utilized, the token-level value baseline is parameterized by a separate GPT2 architecture with a MLP layer on top of the hidden states for each token. Additional details and hyperparameters for our approach are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Analysis</head><p>We will present empirical results showing the effectiveness of ArCHer in Section 5, but in this section we will first highlight some theoretical and conceptual benefits of our hierarchical design. An important for each critic step do 9:</p><p>## Update utterance-level Q and V functions by target function bootstrapping.</p><p>10:</p><formula xml:id="formula_13">𝜃 ← 𝜃 − ∇𝐽 𝜃 (𝑄)</formula><p>◁ Equation <ref type="formula">1</ref>11:</p><formula xml:id="formula_14">𝜓 ← 𝜓 − ∇𝐽 𝜓 (𝑉 ) ◁ Equation 2 or 6 12:</formula><p>## Update target Q and V functions.</p><p>13:</p><formula xml:id="formula_15">θ ← (1 − 𝜏 ) θ + 𝜏 𝜃 14: ψ ← (1 − 𝜏 ) ψ + 𝜏 𝜓 15:</formula><p>end for 16:</p><p>## Update token-level baseline by MC regression. 17:</p><p>for each baseline step do 18:</p><p>𝜂 ← 𝜂 − ∇𝐽 𝜂 ( ̃︀ 𝑉 ) ◁ (Optionally), Equation <ref type="formula" target="#formula_5">4</ref>19:</p><p>end for 20:</p><p>## Update token-level actor with utterance-level critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>for each actor step do 22:</p><p>𝜑 ← 𝜑 − ∇𝐽 𝜑 (𝜋) ◁ Equation 3, 5, or 7</p><p>23:</p><p>end for 24: end for difference between ArCHer and prior token-level RL algorithms such as ILQL <ref type="bibr" target="#b41">(Snell et al., 2023)</ref> is the high-level critic. Thus, we aim to understand the impact of estimation errors in this high-level critic on the token-level policy in contrast with the impact of estimation errors in a token-level critic. While our proof techniques can be easily applied to general high-level and low-level MDPs, we focus on analyzing the specific contrast of utterance and token-level critics to be concrete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditions for convergence.</head><p>To start, we show that a hierarchical RL design prescribed by ArCHer requires substantially weaker conditions for algorithm convergence compared to off-policy token-level methods. These conditions pertain to (1) the capacity of the function class representing the critic (i.e., Bellman completeness <ref type="bibr" target="#b42">(Song et al., 2023;</ref><ref type="bibr" target="#b61">Zhou et al., 2023b;</ref><ref type="bibr" target="#b57">Zanette, 2023;</ref><ref type="bibr" target="#b51">Xie et al., 2021)</ref>), and</p><p>(2) the coverage of off-policy data as measured by the density ratio <ref type="bibr" target="#b59">(Zhan et al., 2022;</ref><ref type="bibr" target="#b7">Foster et al., 2021)</ref>, following the practice standard in RL theory. For (1), we show in Lemma 1 that satisfying the Bellman completeness <ref type="bibr" target="#b42">(Song et al., 2023;</ref><ref type="bibr" target="#b61">Zhou et al., 2023b;</ref><ref type="bibr" target="#b57">Zanette, 2023;</ref><ref type="bibr" target="#b51">Xie et al., 2021)</ref> condition imposes weaker requirements on the function class used to model the critic at the utterance-level (as is the case with ArCHer) as opposed to the token level. Intuitively, this is because a function class representing the token-level critic must exhibit flexibility to realize arbitrary functions at the next token, which would require higher capacity compared to a utterance-level critic that only needs to be able to realize arbitrary functions at the coarser time-scale of utterances.</p><p>For (2), we show in Lemma 2 that the density ratio condition <ref type="bibr" target="#b59">(Zhan et al., 2022;</ref><ref type="bibr" target="#b7">Foster et al., 2021)</ref> imposes identical requirements on the coverage of the offline data for token-level and utterance-level critic, despite a larger space of possible utterances (i.e., actions in the higher-level MDP). Intuitively, this is because the a given offline dataset induces the same trajectory distribution at both the utterance and token levels. In other words, if a trajectory is covered by the offline data at the utterance level, it is also covered by the offline data at the token level, and vice versa.</p><p>Statistical error analysis with finite samples. With the convergence conditions discussed above, we are able to establish an analysis of the statistical error incurred in estimating the advantage estimates using the utterance-level and token-level critics. Intuitively, this means that an utterance-level critic provides a much more correct signal for improving the policy. We state the conclusions of the theorem below, and refer interested readers to Appendix G for formal definitions and proof.</p><p>Theorem 1 (Main Theorem; Informal). For an utterance-level MDP with discount factor 𝛾, where 𝐿 is the maximum length of each utterance, suppose utterance-level Assumption 1 and 2 holds, let 𝑓 be the final Q-function returned by fitted policy evaluation formalized in Algorithm 2 at the utterance level, 𝑓 yields a suboptimality gap of</p><formula xml:id="formula_16">E 𝑠,𝑎∼𝑑 𝜋 [︁ (︀ ( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) [ f (𝑠, 𝑎)]) − 𝐴 𝜋 (𝑠, 𝑎) )︀ 2 ]︁ ≤ 1 𝛾𝐿 1/2 𝒪 (︂ 1 (1 − 𝛾)(1 − 𝛾 1/𝐿 )𝐿 1/2 (𝜖 𝑠𝑡𝑎𝑡 + √ 𝜖 𝑠𝑡𝑎𝑡 )</formula><p>)︂ .</p><p>For an equivalent token-level MDP with discount factor 𝛾 1/𝐿 , suppose token-level Assumption 1 and 2 holds, let 𝑓 be the final Q function returned by Fitted Policy Evaluation formalized in Algorithm 2 at the token level, 𝑓 yields a suboptimality gap of</p><formula xml:id="formula_17">E 𝑠,𝑎∼ ̃︀ 𝑑 𝜋 [︂ (︁ ( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) [ f (𝑠, 𝑎)]) − ̃︀ 𝐴 𝜋 (𝑠, 𝑎) )︁ 2 ]︂ ≤ 𝒪 (︂ 1 (1 − 𝛾)(1 − 𝛾 1/𝐿 )𝐿 1/2 (𝜖 𝑠𝑡𝑎𝑡 + √ 𝜖 𝑠𝑡𝑎𝑡 ) )︂ ,</formula><p>where 𝜖 𝑠𝑡𝑎𝑡 is the statistical error, proportional to 𝑁 −1/2 (𝑁 is the number of utterance-level transitions).</p><p>Informally, Theorem 1 shows that the error in estimating advantages using the token-level critic is 𝛾 √ 𝐿 larger than the the utterance-level critic (in the worst case), where 𝐿 is the maximum number of tokens in each utterance, due to error accumulation. In practice, a common choice for 𝛾 is greater than 0.95 while 𝐿 can be as large as 64 tokens, resulting in 𝛾𝐿 1/2 &gt;&gt; 1. Therefore, we have not only shown that a hierarchical design requires weaker conditions for convergence, but it also enjoys improved guarantees on the statistical error, resulting in a more accurate estimate of policy gradient for improving the policy in the worst case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The goal of our experiments is to evaluate the efficacy of hierarchical RL algorithms derived from ArCHer. Specifically, we aim to answer the following questions: (1) Is ArCHer able to achieve better sample complexity and performance than prior on-policy and off-policy RL methods for LLMs? (2) Does the TD-learning design for the utterance-level critic enable an effective use of off-policy data? (3) How does the performance of ArCHer scale with larger base models (such as Mistral 7B <ref type="bibr" target="#b18">(Jiang et al., 2023)</ref>)? (4) How do different practical algorithms derived from our ArCHer framework compare? To answer these questions, we will present an extensive empirical evaluation of ArCHer and several prior methods on a suite of environments encompassing natural language games, navigation problems posed in natural language, and interaction with the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tasks and Environments</head><p>To stress-test the efficacy of ArCHer, we need environments and task setups that satisfy several desiderata. First, the chosen tasks must require strategic multi-step planning and reasoning under delayed rewards, and cannot be solved in one turn. We also want these tasks to require LLMs to generate coherent natural language and keep the task realistics.</p><p>Next, we want these tasks to be solvable by models of upto 7 billion parameter scale, which corresponds to the upper end of our computational budget. Finally, the chosen tasks should support fast and reliable evaluations, for reproducibility and benchmarking. Most existing LLM agent tasks such as those which require interacting with terminals, operating systems, and databases <ref type="bibr" target="#b52">(Yang et al., 2023a;</ref><ref type="bibr" target="#b24">Liu et al., 2023)</ref> require larger base models (typically larger than 7B) for obtaining non-trivial success rates and can often be solved in a single step. This makes these tasks unfavorable for fast iteration within our compute budget. Other dialogue and tutoring tasks <ref type="bibr" target="#b15">(Hong et al., 2023;</ref><ref type="bibr" target="#b47">Verma et al., 2022;</ref><ref type="bibr" target="#b62">Zhu et al., 2020;</ref><ref type="bibr" target="#b21">Lee et al., 2019;</ref><ref type="bibr" target="#b2">Budzianowski et al., 2018)</ref> require either costly user studies or evaluate using metrics that do not directly represent task performance, making them unfavorable for stress-testing our approach. Therefore, we utilize a different set of tasks for our evaluations.</p><p>Concretely, we consider the following tasks: (1) Detective Game <ref type="bibr" target="#b14">(Hausknecht et al., 2019)</ref>, an interactive text fiction game where the agent must generate a sequence of natural language actions (e.g.,"take paper", "eat apple") based on environment feedback. A reward is given if the agent reaches some milestones towards finishing the game, where the end goal is to successfully find the murderer in a murder mystery; (2)Twenty Questions <ref type="bibr" target="#b0">(Abdulhai et al., 2023)</ref>, a dialogue task where the agent plays the role of a guesser trying to guess a hidden word from a list of 157 words within twenty yes/no questions. The oracle answers the questions with "Yes.", "No.", or "Invalid Question." The oracle is simulated with a "flan-t5-small" (Chung et al., 2022) model trained with supervised fine-tuning on the dataset provided by <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref>. Upon guessing the correct word, the agent receives a reward of 0 and the environment terminates. Otherwise, a reward of -1 is provided at each time step. We also study a variation of this task with a list of only 10 possible underlying words, that we call Twenty Questions Subset. This variant challenges the algorithms to tailor a very specific strategy when there is a shortcut in the task; (3) Guess My City <ref type="bibr" target="#b0">(Abdulhai et al., 2023)</ref>, a similar multi-turn task where the agent attempts to guess the name of a hidden city from a list of 100 cities within twenty questions. A crucial difference between the Guess My City task and the Twenty Questions task is that the guesser is now allowed to ask any question and can observe free-form responses (which are not necessarily "Yes" or "No"); (4) WebShop <ref type="bibr" target="#b54">(Yao et al., 2023a)</ref>, a tool-use task where the agent is instructed to buy an item from a shopping server. A dense reward between 0 and 1 is provided based on the similarity of the item purchased and the item requested. See Appendix A for more details. These tasks require planning over long horizons, allow non-trivial success rates within the 7 billion parameter scale, and come equipped with reproducible and task-directed evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons and Baseline Approaches</head><p>We compare our method to a number of alternative RL approaches. For token-level methods, we consider token-level PPO <ref type="bibr" target="#b39">(Schulman et al., 2017)</ref> due to its state-of-the-art performance with LLMs <ref type="bibr" target="#b29">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b34">Ramamurthy et al., 2022)</ref>. For each iteration, PPO collects new on-policy data by rolling out the current actor and uses these data to estimate the policy gradient. Perhaps most importantly, data collected by previous policies is simply discarded for later updates. We use the existing PPO implementation by <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref>. We also implemented a token-level DQN <ref type="bibr" target="#b26">(Mnih et al., 2013)</ref> method, but were unable to get it to attain non-zero task performance. Neither did we find any prior work evaluating this method, and hence we omit it from our results.</p><p>We also considered a non-RL approach based on filtered behavioral cloning (BC), denoted as Filtered BC. As prior work <ref type="bibr" target="#b0">(Abdulhai et al., 2023;</ref><ref type="bibr" target="#b41">Snell et al., 2023)</ref> shows, perhaps surprisingly, this simple baseline often attains competitive or better performance than RL approaches for LLMs, implying that outperforming filtered BC is a hallmark of proper functioning of the "RL component" in an approach. Our implementation of filtered BC maintains a fixed size buffer of recent rollouts and trains the actor with an imitation learning loss on the top 10% rollouts, as identified based on the totaltask reward.</p><p>For utterance-level methods, we compare with a state-of-the-art utterance-level RL method, CHAI <ref type="bibr" target="#b47">(Verma et al., 2022)</ref>. CHAI was designed for offline RL specifically. To extend it to learn from online rollouts, we simply replace the pessimistic loss function (i.e., a conservative Q-learning <ref type="bibr" target="#b21">(Kumar et al., 2020)</ref> loss) in this approach with a standard TD-learning loss function on data sampled from the off-policy replay buffer, identical to the one used in ArCHer. That said, the key difference is that CHAI utilizes a frozen actor obtained by behavioral cloning (or supervised fine-tuning) on the replay buffer, whereas ArCHer optimizes the actor as well. Each time when an action needs to be sampled, 𝑘 utterances are sampled from the frozen actor. The utterance-level critic in CHAI ranks these 𝑘 utterances and chooses the utterance with the highest Q value. A larger value of 𝑘 would likely lead to better performance, but is also computationally expensive for this method. To obtain a sweet spot, we instantiate CHAI with 𝑘 = 5 to effectively balance computational overhead and performance (note that 𝑘 = 5 already results in a runtime of about 4 times longer than our approach for this prior method; going beyond would be prohibitive for our computational resources). Model architectures, learning rates, and other algorithm-agnostic details are kept identical between all prior methods and ArCHer (see Section 3.6).</p><p>Finally, akin to single-turn RL fine-tuning of LLMs, we initialize the token-level policy for all methods with a policy checkpoint obtained by running supervised instruction tuning on sub-optimal data for the task (see Appendix A for how this sub-optimal data is generated). Uniformly across all methods, this initialization enables effective exploration at the beginning of RL fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guess My City</head><p>Figure <ref type="figure">3</ref>: Online RL results comparing ArCHer and other approaches on four tasks. We plot the median performance of each method across three seeds. Observe that ArCHer steadily improves the policy, outperforming all other methods on three tasks and matching the best prior approach on the simple Detective Game task. While PPO appears to not be learning, by zooming into the learning curve in Figure <ref type="figure" target="#fig_7">7</ref>, we find that PPO still gradually improves but at a very slow speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results: Sample Efficiency in the Online Setting</head><p>Figure <ref type="figure" target="#fig_4">3 and 4</ref> show the comparison between ArCHer with other methods across the five tasks. We also provide some example rollouts of ArCHer for each environment in Appendix E. Overall, we found that ArCHer converges to a much better performance than all other prior methods on the four harder tasks that require identifying hidden information or present diverse initial states (i.e., Twenty Questions Subset, Twenty Questions, Guess My City, and WebShop). In fact, on WebShop, online RL training of GPT2 base model via ArCHer outperforms several effective prompting strategies (i.e., an expert-written prompt and ReAct <ref type="bibr" target="#b55">(Yao et al., 2023b)</ref>) applied on top of GPT-3.5, a strong LLM.</p><p>First of all, we found that token-level PPO fails to achieve performance competitive with all other off-policy methods using the same amount of data. This is perhaps unsurprising, as PPO is an on-policy method, and therefore, cannot effectively reuse samples from past iterations. In particular, on the Twenty Questions task, we observed that PPO could only stably improve when provided with at least 1024 on-policy rollouts for each gradient step, likely because of high gradient variance. This observation corroborates the finding of <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref>, suggesting that online PPO is less practical for this task. Quantitatively, we find that while it takes more than 100k samples for PPO to attain an average return just higher than -17 (see Figure <ref type="figure" target="#fig_7">7</ref> in the appendix), ArCHer attains this reward value with fewer than 1000 samples, implying at least a 100x boost in sample efficiency.</p><p>While filtered BC generally converges very quickly, the resulting policy often performs suboptimally and does not improve with more data in Figure <ref type="figure">3</ref>. On the other hand, ArCHer enjoys steady policy improvement as it collects more samples. Finally, we observed that while CHAI improved at a faster than ArCHer initially, it often converged to a worse final performance. We suspect this is because the critic in CHAI is directly used to rerank samples from a frozen behavior policy, which only enables a narrow margin for policy improvement. On the contrary, ArCHer needs an initial learning phase to reduce critic estimation error, after which it can improve steadily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study: Importance of Off-Policy Data</head><p>In Figure <ref type="figure" target="#fig_5">5</ref> (b), we investigate the importance of off-policy data by varying the size of replay buffer on the Guess My City task. A smaller replay buffer means that updates rely on repeatedly sampling on-policy data. In our experiments, we observed that using a replay buffer containing only the most recent 48 rollouts resulted in unstable learning, likely due to overfitting on limited data, which has been observed in standard RL problems outside of LLMs <ref type="bibr" target="#b27">(Nikishin et al., 2022)</ref>. On the other hand, larger buffers are more stable. However, increasing the size of the buffer beyond a certain point is benign, resulting in no meaningful changes to performance. Overall, this means that making use of off-policy data can improve the stability and performance of practical methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study: Alternate Base Models for the High-Level Critic in ArCHer</head><p>In Figure <ref type="figure" target="#fig_5">5</ref> (c), we carried out an ablation of changing the architecture for the critic model from an encoder-only RoBERTa <ref type="bibr" target="#b25">(Liu et al., 2019)</ref>  2019), where we took the embedding of the last "[EOS]" token as the embedding of the utterances.</p><p>Observe that although ArCHer w/ RoBERTa critic learns a bit faster in the beginning, learning curves for both of these critic models behave identically past a certain number of initial samples. Therefore, ArCHer can also use decoder-only transformer models, with no loss in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study: Scaling the Base Model from 100M to 7B Parameters</head><p>In Figure <ref type="figure" target="#fig_5">5</ref> (d), we replaced the 100 million parameter GPT-2 model used to represent the token-level actor in ArCHer with a 7 billion parameter Mistral model <ref type="bibr" target="#b18">(Jiang et al., 2023)</ref>. When using this 7B model, we did not need to apply supervised fine-tuning since the open-source checkpoint already attained non-trivial rewards on Twenty Questions Subset when evaluated zero-shot. Observe in Figure <ref type="figure" target="#fig_5">5</ref> (d), that ArCHer with this Mistral7B actor learns to solve the task much faster than ArCHer with a GPT2 Actor. This indicates that our ArCHer framework can scale well with LLMs with more parameters. More broadly, due to similarities between the token-level actor update in ArCHer and single-turn RL fine-tuning for LLMs in RLHF, we would expect performance to exhibit similar benefits from scaling the model size for the policy <ref type="bibr" target="#b9">(Gao et al., 2023)</ref>. This finding corroborates this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Alternate Practical Algorithms Derived from ArCHer</head><p>Online ArCHer with improved policy gradient estimators. In Figure <ref type="figure" target="#fig_5">5</ref> (a), we compare the performance of ArCHer with and without a token-level baseline on the Guess My City task (Equation <ref type="formula" target="#formula_7">5</ref>). This task requires the utterances of the agent in each turn to be longer and more diverse than other tasks. Observe that incorporating this token-level baseline in ArCHer outperforms standard ArCHer by a large margin, supporting our hypothesis that the introduction of the token-level baseline can effectively reduce the variance of the vanilla policy gradient while updating the token-level actor (especially when the utterances are long and diverse). That said, this improvement requires paying an extra computational overhead associated with training Ṽ𝜂 , which might not be necessary when each utterance is short. Overall, this study illustrates one of the central benefits of our hierarchical design: we can choose the best method for the higher and lower level based on their distinct requirements.</p><p>Offline ArCHer with IQL and AWR. We now present a preliminary study of ArCHer in the offline setting, when learning from a static dataset from past environment interactions. Due to computational constraints, we were not able to perform extensive comparisons with the state-of-the-art algorithms; rather, we investigated the effect of several design choices in order to investigate the effect of various design choices in the offline setting, including IQL and AWR losses described in Section 3.5. We also incorporate a baseline, BC, which performs (unfiltered) imitation learning on the offline dataset. Finally, we also ran filtered BC, which only imitates the best trajectories in the offline dataset.</p><p>In Table <ref type="table" target="#tab_2">1</ref>, we also evaluate several other design choices in the offline setting. Directly borrowing the REINFORCE objective from the online setting (Equation <ref type="formula">3</ref>) results in a quick collapse of performance due to the lack of any regularization to prevent out-of-distribution actions, as is well known in the offline RL problem setting outside of LLMs <ref type="bibr" target="#b21">(Kumar et al., 2019)</ref>. Combining Equation 3 with an imitation learning loss stabilizes learning and results in a performance improvement, but still underperforms advantage-weighted regression (AWR) <ref type="bibr" target="#b31">(Peng et al., 2019)</ref>. Finally, we replaced the IQL in-sample expectile backup with a SARSA backup, where the utterance present in the offline dataset at the next turn is used to compute the Bellman target, i.e., no implicit or explicit maximization over target values is utilized, and the value function is trained to represent the long-term Q-values of the data collection policy. Observe that this variant did not offer the same level of policy improvement as using IQL to train the critic in this setting. This highlights the importance of maximization over actions to calculate Bellman targets in the offline setting. Finally, we also find that instantiations of ArCHer that use IQL and SARSA in conjunction with AWR, both outperform the naïve BC and filtered BC, further higlighting the importance of dynamic programming to train the critic.</p><p>To summarize, our experiments show that ArCHer can be used to derive multi-turn RL methods that lead to substantial improvements to sample efficiency of LLM policy training, benefit from offline and off-policy experience as well as improvements to RL algorithms, and scale with model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>In this paper, we propose a novel Actor-Critic Framework with a Hierarchical Structure (ArCHer) for multi-turn LLM agent tasks. By running two RL algorithms simultaneously, one at the high level (i.e., utterances in our practical method) and one at the low level (i.e., tokens), ArCHer reduces task horizons while enjoying the ability to retain a compact token-level action space at the low level. These characteristics yield a more practical, efficient, and effective method for training LLMs to be effective decision-makers or agents. ArCHer is simple and extensible, and can be flexibly instantiated with a variety of components for both the high-and low-level methods. Empirically, we observed that ArCHer significantly outperforms prior RL methods for LLMs, on a range of online RL tasks and scales favorably with more capable base models and other design improvements.</p><p>Due to the computational constraints associated with running many trials of multiple RL algorithms and ArCHer instantiations, we had to conduct most of our experiments with a relatively small GPT-2 architecture. While our result with a Mistral7B base model demonstrates favorable scaling properties of our approach but rigorously evaluating our method with larger models (and on other benchmarks) is an important direction for future work. Our evaluations also focus entirely on tasks with computational rewards, and our method still requires a significant number of interactions (in the thousands), so an important future direction is to study how such methods can be made feasible to learn from live interactions with humans, when only about 100 interactions are available. We believe that model-based RL approaches could be quite promising here. Finally, deriving and evaluating novel practical algorithms from the hierarchical framework of ArCHer is also an interesting avenue for future work with the potential to greatly improve task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A. Environment and Dataset Details</head><p>In this section, we provide more details on the environments and datasets that we used in our experiments. Example actions and observations for each environment are shown in Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>Detective Game <ref type="bibr" target="#b14">(Hausknecht et al., 2019)</ref>. In this game, the agent plays the role of a detective looking into a case where the Mayor got murdered. At each time step, the agent generates a free-form text where the game engine parses the text and determines the next state at each time step. The game engine provides a feedback of "Illegal Action." if the generated text is an illegal action or cannot be correctly parsed. The optimal policy takes 51 steps to solve and reaches a maximum reward of 360.</p><p>The game timeouts and terminates after 60 steps (including steps where illegal actions are generated).</p><p>The observation at each time step includes the current surroundings, items carried, environment feedback for the outcome of the last action, and a list of available actions. The Supervised Fine-Tuning (SFT) dataset for this environment consists of 1000 trajectories of an agent picking a random action from the list of available actions at each timestep. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions:</head><p>What is the primary language spoken in your city?</p><p>The primary language spoken in the city is Spanish.</p><p>Is your city located on the east coast of the country? Yes, it is located on the east coast of the country.</p><p>Is your city located in North America? No, my city is not located in North America.  Twenty Questions and Twenty Questions Subset <ref type="bibr" target="#b0">(Abdulhai et al., 2023)</ref>. In this environment, for each episode, a random word is chosen from a list of 157 words of household items such as "basketball", "apple", and "car". The word is held hidden from the agent and the agent is tasked to guess the hidden word within 20 questions. The questions are limited to yes/no questions and the answers from the oracle are limited to "Yes.", "No.", and "Invalid Question.". As opposed to using "flan-t5-xl" <ref type="bibr">(Chung et al., 2022)</ref> as the oracle <ref type="bibr" target="#b0">(Abdulhai et al., 2023)</ref>, we train a 'flan-t5-small" to simulate the oracle with the same data and use it for our online experiments due to computational constraints. The agent gets a reward of 0 if it guesses the correct word and the episode terminates.</p><p>Otherwise, the agent gets a reward of -1 for each question it raises. This reward structure results in a minimum reward of -20 if the agent does not guess the corret word with twenty questions and a maximum reward of 0 if the agent guesses the correct word with the first question although it is very unlikely. We use the official offline dataset provided by <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref> with 100K simulated episodes. Our SFT checkpoints for online experiments for both Twenty Questions and Twenty Questions Subset are also trained with this dataset. Twenty Questions Subset keeps everything else the same except that it uses a subset of 10 hidden words in the word list. Since the offline dataset and the SFT checkpoint for online experiments are based on the entire Twenty Questions, Twenty Questions Subset challenges different algorithms with a significant distribution shift and requires the agent to come up with an entirely different strategy from behavior cloning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guess My City (Abdulhai et al., 2023</head><p>). This environment is a similar dialogue task to Twenty Questions. For each episode, a random city is chosen from a list of 100 cities in the world. The city is held hidden from the agent and the agent is tasked to guess the name of the city within 20 questions.</p><p>Both the questions and answers are free-form except that the answers are not allowed to contain the name of the city. As opposed to using "flan-t5-xl" <ref type="bibr">(Chung et al., 2022)</ref> as the oracle <ref type="bibr" target="#b0">(Abdulhai et al., 2023)</ref>, we train a 'flan-t5-small" to simulate the oracle with the same data and use it for our online experiments due to computational constraints. We found in our online experiments that the agent can easily learn to "exploit" the oracle by tricking it to directly output the name of the city. Therefore, we simply replace the answer with a hardcoded template "I cannot answer that question." if the name of the city is found in the output of the oracle language model to reduce reward hacking. The reward structure is the same as Twenty Questions. We use the official offline dataset provided by <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref> with 100K simulated episodes.</p><p>Web Shopping <ref type="bibr" target="#b54">(Yao et al., 2023a)</ref>. This environment challenges the ability of the agents to interact with external tools. For each episode, a random instruction requesting a specific item is chosen and shown to the agent. The agent needs to make use of a simplified web shopping server to make the purchase. Every successful purchase is consisted of searching the keywords in the search engine, selecting an item from searched results, clicking on features and attributes for the item, and finally making the purchase. Following ReAct <ref type="bibr" target="#b55">(Yao et al., 2023b)</ref>, the agent can choose to take a "think" action before taking any actual actions such as "search" and "click". An observation consists of the instruction and the history of visited webpages (described in text) and actions. The reward is a scalar between 0 and 1 depending on the similarity of the purchased item with the requested item. For example, a partial reward will be given if the agent purchases a black king-sized bed while a black queen-sized bed is requested. The episode timeouts after 10 interaction steps and a reward of 0 is issued. Our main online environments use a subset of 100 instructions from index 2000 to 2100 for a fast evaluation. We collect the offline dataset using the instructions from index 0 to 1000 with GPT-3 text-davinci-002 with prompts from ReAct's official implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Offline Algorithm and Practical Considerations</head><p>Our offline algorithm is a hierarchical version of the IQL algorithm <ref type="bibr" target="#b20">(Kostrikov et al., 2021)</ref>. Specifically, the critic leverages IQL (Eq. ( <ref type="formula" target="#formula_9">6</ref>)) while the actor update is based on AWR (Equation <ref type="formula" target="#formula_11">7</ref>).</p><p>These choices for the actor and for the critic update identify two key hyperparameters, the expectile value 𝜏 (defined in Equation 6 and 7) and the temperature 𝛽, whose effect is described in the respective sections. These hyper-parameters are already present in the original IQL algorithm <ref type="bibr" target="#b20">(Kostrikov et al., 2021)</ref>, and they have a similar interepretation here. By choosing 𝜏 and 𝛽 appropriately, the algorithm identifies a policy whose performance should be between the optimal one and the one that generated the dataset. (In general, recovering the optimal policy by just using a dataset may not be possible as the dataset may not contain information about an optimal policy).</p><p>The offline algorithm shares most of the ingredients with its online counterpart, such as the double critic, target networks, soft updates, and value function heads. However, some unique features inherited from IQL allow to considerably simplify several algorithmic choices.</p><p>• The actor and the critic no longer need to be synchronized by using a certain update ratio. This is because the critic update defined in Equation 6 is independent of the actor's current policy, and so the two can be updated with any desired frequency without introducing instabilities. • It is not necessary to pre-train the policy with a behavioural cloning objective, because such objective is already included in the actor's loss function in Equation <ref type="formula" target="#formula_11">7</ref>. • The warmup steps for the critic are also not necessary, because the initially small advantage function has a neglegible effect in the AWR loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Baseline Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Performance of PPO</head><p>In Figure <ref type="figure" target="#fig_7">7</ref>, we provide a zoom-in of the learning curves of PPO for Twenty Questions, Twenty Questions Subset, and Guess My City. We observed that PPO does improve over the SFT checkpoint, especially in the more simple task Twenty Questions Subset. However, as PPO is unable to reuse past off-policy data, we need to collect at least 1024 trajectories of on-policy data for each PPO update, as shown in Appendix F. This observation is consistent with <ref type="bibr" target="#b0">Abdulhai et al. (2023)</ref>.  </p><formula xml:id="formula_18">0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Additional Reproduction Details for WebShop Experiment</head><p>For our WebShop experiment, we utilized the environment and the few-shot prompting baselines from ReAct <ref type="bibr" target="#b55">(Yao et al., 2023b)</ref>. ReAct introduces two prompting strategies: the ReAct method and the Act-only method, which we denote as ReAct and expert-prompt in Figure <ref type="figure" target="#fig_4">4</ref>. The ReAct method additionally allows the agent to articulate its reasoning before making an action, whereas the Act-only method does not. We use the original prompts in ReAct's implementation without any modifications, and to ensure that our paper is self-contained, we also include these prompts in Figure <ref type="figure" target="#fig_10">9 and 8</ref>.</p><p>We assessed these algorithms on the webshop index from 2000 to 2100 to establish the LLM-based few-shot baselines in Figure <ref type="figure" target="#fig_4">4</ref>. Notably, by early January 2024, OpenAI had deprecated the GPT-3 text-davinci-002 model used in the original ReAct study. Consequently, we switched to the gpt-3.5turbo-instruct model. While the expert-prompt approach yielded performance comparable to the best performing text-davinci-002 based method, the ReAct method underperformed significantly with the newer model. This discrepancy, also observed and replicated in other tasks independetly by other researchers<ref type="foot" target="#foot_0">1</ref> , is faithfully depicted in Figure <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReAct Prompt</head><p>Webshop Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. TD-Learning v.s. MC Regression</head><p>To validate whether TD-learning plays an important role in ArCHer, we carried out an ablation study where we replaced TD-learning in ArCHer with MC regression for critic updates. To make sure that data in the replay buffer are generated by similar policies, we use a smaller replay buffer that contains trajectories collected by three latest policies. The ablation results are shown in Figure <ref type="figure" target="#fig_1">10a</ref>. We observed that MC regression may learn faster in the beginning as the information propagates faster than per step TD learning, but it fails to learn reliably over then entire training process. This ablation result shows the importance of TD-learning to effectively make use of off-policy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Online IQL Critic Loss</head><p>In our ablation study of using IQL critic loss in the online setting, we set 𝜏 = 0.9 to encourage more risk-seeking for better explorations. As we can see from Figure <ref type="figure" target="#fig_1">10b</ref>, the use of IQL critic can indeed accelerate explorations in simple tasks such as Detective Game. However, a naive instantiation of IQL fails to provide an unbiased estimate for the policy gradient resulting in potential instabilities in harder tasks such as Twenty Questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Reward Hacking</head><p>To understand whether our agent has really learnt to behave more strategically with reinforcement learning instead of exploiting the environment, we present sample trajectories for Twenty Questions Subset, Twenty Questions, Guess My City, and WebShop in Figure <ref type="figure" target="#fig_1">11</ref>, 12, 13, 14. Note that for detective game, there is only a limited number of legal actions and all illegal actions will lead to making no progress in the game, resulting in a lower rewards. For Twenty Questions Subset and Twenty Questions, we observed that the successful dialogues tend to be mostly natural and strategic while some unsuccessful dialogues tend to contain more meaningless repetitions. For Guess My City, we observed that the agent learnt to ask the predominant language of the city which is one of the most informative questions in the SFT distribution we considered in Appendix A. However, as there are some vulnerabilities and hallucinations of the imperfect oracle model for generating free-form answers, some of the conversations might diverge to meaningless repetitions in the end. For WebShop, as the environment forces the format of legal actions for the web server to process, the trajectories</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guess My City</head><p>Questions: What is the predominant language spoken: The predominant language spoken is Mandarin Chinese. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Hyperparameters</head><p>Table <ref type="table" target="#tab_7">2</ref> shows hyperparameters for ArCHer and other baselines for all environments. As shown in the table, most hyperparameters are held the same across all environment except that a smaller actor learning rate is used for Twenty Questions and a larger number of rollout trajectories is used for Web Shopping. This shows that ArCHer is relatively insensitive to selection of hyperparameters and does not require too much hyperparameter tuning to work in different environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proof of Main Theorem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Equivalent Utterance and Token Level MDPs</head><p>We consider the groundtruth utterance-level discounted infinite horizon MDP ℳ = {𝒮, 𝒜, 𝛾, 𝑟, 𝜇 0 , 𝑃 } as defined in Section 3.1. An equivalent token-level infinite horizon MDP can be constructed with ︁ ℳ = { ̃︀ 𝒮, ̃︀ 𝒜, 𝛾 1/𝐿 , 𝑟, 𝜇 0 , ̃︀ 𝑃 }, where 𝐿 is the length of each utterance, ̃︀ 𝒜 contains all individual tokens with a special padding token (such that each utterance is padded to the same length 𝐿), ̃︀ 𝒮 ∈ 𝒮 × ̃︀ 𝒜 𝐿 contains both the state in the utterance-level MDP and the partial sequence of utterance that has already been generated, 𝛾 1/𝐿 is the equivalent discount factor. Note that this definition of token-level MDP is not the same as the definition of token-level MDP in the hierarchical language MDP defined for ArCHer in Section 3.1. The token-level MDP for ArCHer is only embedded in one particular utterance and the only reward that it receives is at the end of the utterance from the utterance-level critic while the equivalent token-level MDP spans multiple utterances and receives rewards from the external environment at the end of each utterance. This construction of token-level MDP is "equivalent" to the utterance-level MDP in the sense that for any autoregressive policy 𝜋 that generates an utterance token by token, we have:</p><formula xml:id="formula_19">∀𝑠 ∈ 𝒮, 𝑉 𝜋 (𝑠) = ̃︀ 𝑉 𝜋 (𝑠),</formula><p>where 𝑉 𝜋 and ̃︀ 𝑉 𝜋 are value functions of 𝜋 in the utterance-level MDP ℳ and the token-level MDP ︁ ℳ respectively. We use ̃︀ 𝜋 for the same utterance-level policy 𝜋 when it generates one token at a time.</p><p>As usual, for any MDP ℳ, we define 𝑑 𝜋 ∈ ∆(𝒮 × 𝒜) as the average state-action occupancy measure of policy 𝜋 such that:</p><formula xml:id="formula_20">𝑑 𝜋 (𝑠, 𝑎) = (1 − 𝛾)(𝜇 0 (𝑠)𝜋(𝑎|𝑠) + ∞ ∑︁ 𝑡=1 𝛾 𝑡 𝑃 𝜋 (𝑠 𝑡 = 𝑠, 𝑎 𝑡 = 𝑎))</formula><p>We denote 𝑉 𝜋 = E 𝑠 0 ∼𝜇 0 𝑉 𝜋 (𝑠 0 ) as the expected total discounted reward of 𝜋. We denote 𝒯 𝜋 as the Bellman operator associated with 𝜋, i.e., given a function 𝑓 ∈ 𝒮 × 𝒜 ↦ → R, we have</p><formula xml:id="formula_21">𝒯 𝜋 𝑓 (𝑠, 𝑎) = 𝑟(𝑠, 𝑎) + 𝛾E 𝑠 ′ ∼𝑃 (𝑠,𝑎),𝑎 ′ ∼𝜋(𝑠 ′ ) [︀ 𝑓 (𝑠 ′ , 𝑎 ′ ) ]︀ .</formula><p>Similar definitions can be made in the token-level MDP ̃︁ ℳ for ̃︀ 𝑑 𝜋 , ̃︀ 𝑉 𝜋 , and ̃︀ 𝒯 𝜋 . We also define ̃︀ 𝐴 𝜋 as the advantage function in the token level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Fitted Policy Evaluation Subroutine</head><p>In this section, we present our theoretical subroutine for fitting the critic in Algorithm 2. On a high level, it just repeats finding the Q function that minimizes the bellman error with respect to the Q function in the last iteration, and returns the average of all Q functions in the end. Both critic fitting in the token-level MDP ̃︁ ℳ or in the utterance-level MDP ℳ follows from the same subroutine with the same function class ℱ that map the space of seuqnces of tokens to real values. This theoretical algorithm is simply a more fomalized version of the critic update in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Assumptions</head><p>We present the two important assumptions that we use for analyzing FPE subroutine, and both assumptions share the same definition for utterance-level MDP ℳ and token-level MDP ̃︁ ℳ.</p><p>Assumption 1 (Bellman Completeness <ref type="bibr" target="#b42">(Song et al., 2023;</ref><ref type="bibr" target="#b61">Zhou et al., 2023b;</ref><ref type="bibr" target="#b57">Zanette, 2023;</ref><ref type="bibr" target="#b51">Xie et al., 2021)</ref>). We say that ℱ is Bellman Complete for some policy 𝜋, if for all 𝑓 ∈ ℱ, there exists a</p><formula xml:id="formula_22">𝑓 ′ ∈ ℱ such that ‖𝑓 ′ (𝑠, 𝑎) − 𝒯 𝜋 𝑓 (𝑠, 𝑎)‖ ∞ = 0.</formula><p>Assumption 2 (Density Ratio Coefficient <ref type="bibr" target="#b59">(Zhan et al., 2022;</ref><ref type="bibr" target="#b7">Foster et al., 2021)</ref>). Given the offline distribution 𝜈, for any policy 𝜋, we define the density ratio coefficient as 𝐶 𝜈,𝜋 := max Solve the square loss regression problem to compute:</p><formula xml:id="formula_23">𝑓 𝑘 ← argmin 𝑓 ∈ℱ ̂︀ E 𝒟 𝑘 [(𝑓 (𝑠, 𝑎) − 𝑟 − 𝛾𝑓 𝑘−1 (𝑠 ′ , 𝜋(𝑠 ′ ))) 2 ] (8) 4: end for 5: Return f = 1 𝐾 ∑︀ 𝐾 𝑘=1 𝑓 𝑘 .</formula><p>The following two lemmas compare the utterance-level assumptions and token-level assumptions.</p><p>Lemma 1. For any stationary policy 𝜋, token-level Bellman Completeness for ̃︁ ℳ =⇒ utterance-level Bellman Completeness for ℳ Proof. ∀𝑠 ∈ 𝒮, 𝑎 ∈ 𝒜 being state and action in the utterance-level, the utterance 𝑎 can be decomposed into 𝐿 tokens in the action space of the token-level MDP:</p><formula xml:id="formula_24">𝑎 = ̃︀ 𝑎 1:𝐿 , ̃︀ 𝑎 𝑖 ∈ ̃︀ 𝒜, 𝑖 = 1, . . . , 𝐿.</formula><p>Therefore,</p><formula xml:id="formula_25">min 𝑓 ′ ∈ℱ |𝑓 ′ (𝑠, 𝑎) − 𝒯 𝜋 𝑓 ′ (𝑠, 𝑎)| = min 𝑓 1 ,...,𝑓 𝐿 ∈ℱ |𝑓 1 (𝑠, 𝑎) − ̃︀ 𝒯 𝜋 𝑓 2 (𝑠, 𝑎) + 𝑟(𝑠, 𝑎) + 𝛾 1/𝐿 E 𝑠 ′ ∼𝑃 (•|𝑠,𝑎),̃︀ 𝑎 1 ∼̃︀ 𝜋(•|𝑠 ′ ) 𝑓 2 (𝑠 ′ , ̃︀ 𝑎 1 ) − 𝛾 1/𝐿 E 𝑠 ′ ∼𝑃 (•|𝑠,𝑎),̃︀ 𝑎 1 ∼̃︀ 𝜋(•|𝑠 ′ ) ̃︀ 𝒯 𝜋 𝑓 3 (𝑠 ′ , ̃︀ 𝑎 1 ) + • • • + 𝛾 (𝐿−1)/𝐿 E 𝑠 ′ ∼𝑃 (•|𝑠,𝑎),̃︀ 𝑎 1:𝐿−1 ∼̃︀ 𝜋(•|𝑠 ′ ) 𝑓 𝐿 (𝑠 ′ , ̃︀ 𝑎 1:𝐿−1 ) − 𝑟(𝑠, 𝑎) − 𝛾 (𝐿−1)/𝐿 E 𝑠 ′ ∼𝑃 (•|𝑠,𝑎),̃︀ 𝑎 1:𝐿−1 ∼̃︀ 𝜋(•|𝑠 ′ ) ̃︀ 𝒯 𝜋 𝑓 (𝑠 ′ , ̃︀ 𝑎 1:𝐿−1 )| ≤ min 𝑓 1 ,...,𝑓 𝐿 ∈ℱ |𝑓 1 (𝑠, 𝑎) − ̃︀ 𝒯 𝜋 𝑓 2 (𝑠, 𝑎)| 𝐿 ∑︁ 𝑖=2 𝛾 (𝑖−1)/𝐿 E 𝑠 ′ ∼𝑃 (•|𝑠,𝑎),̃︀ 𝑎 1:𝑖−1 ∼̃︀ 𝜋(•|𝑠 ′ )) |𝑓 𝑖 (𝑠, ̃︀ 𝑎 1:𝑖−1 ) − ̃︀ 𝒯 𝜋 ((𝑠, ̃︀ 𝑎 1:𝑖−1 )| ≤0,</formula><p>where the last inequality follows from the assumption of token-level Bellman Complete.</p><p>The next lemma assumes that the offline distribution 𝜈 is the same for both token-level MDP and utterance-level MDP, i.e. the token-level transitions are derived by splitting utterance-level transitions. We use ̃︀ 𝜈 to denote the token-level offline distribution created in this way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Proof of Main Theorem</head><p>Now we are ready to analyze the sample complexity of Fitted Policy Evaluation in Algoritm 2, the proof of our main theorem makes use of several technical lemmas from Section G.5.</p><p>Lemma 3 (Guarantees of FPE). For the algorithm described in Algorithm 2 with 𝐾 independent datasets 𝒟 1:𝐾 = {(𝑠, 𝑎, 𝑟, 𝑠 ′ )} such that the effective number of samples 𝑁 = 𝑀 𝐾, if the function class satisfies max 𝑓 ∈ℱ ,𝑠∈𝒮,𝑎∈𝒜 |𝑓 (𝑠, 𝑎)| &lt; 𝑅, then with probability at least 1 − 𝛿 the output value function satisfies:</p><formula xml:id="formula_26">E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] ≤ 𝐶 𝜈,𝜋 (︂ 4(𝑅 + 1) 2 𝐾 + 256𝑅 2 log(2|ℱ|𝐾/𝛿) 𝑀 )︂ .</formula><p>Proof. By applying Lemma 6, to each iteration 1, . . . , 𝐾, and apply a union bound over all iterations, we have that:</p><formula xml:id="formula_27">∀𝑘, 𝑀 E 𝑠,𝑎∼𝜈 [(𝑓 𝑘 (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝑘−1 (𝑠, 𝑎)) 2 ] ≤ 3𝑀 inf 𝑓 ∈ℱ E 𝑠,𝑎∼𝜈 [(𝑓 (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝑘−1 (𝑠, 𝑎)) 2 ] + 256𝑅 2 log(2|ℱ|𝐾/𝛿) ∀𝑘, E 𝑠,𝑎∼𝜈 [(𝑓 𝑘 (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝑘−1 (𝑠, 𝑎)) 2 ] ≤ 256𝑅 2 log(2|ℱ|𝐾/𝛿) 𝑀 ,<label>(9)</label></formula><p>where the second line holds by Assumption 1. To combine the guarantees that we have from each round to the guarantee of the returned value function f :  </p><formula xml:id="formula_28">E 𝑠,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Technical Lemmas</head><p>First, we would like to prove an interesting lemma that allows us to compare the different discount factors for token level 𝛾 1/𝐿 and utterance level 𝛾.</p><p>Lemma 4. With the discount factor 𝛾 ∈ (0, 1), and 𝐿 being a positive number, we have that:</p><formula xml:id="formula_29">1 1 − 𝛾 = 1 (1 − 𝛾 1/𝐿 )(1 + 𝛾 1/𝐿 + 𝛾 2/𝐿 + • • • + 𝛾 (𝐿−1)/𝐿 ) ≤ 1 𝐿𝛾(1 − 𝛾 1/𝐿 )</formula><p>Below are some common technical lemmas useful for reinforcement learning.</p><p>Lemma 5 <ref type="bibr">((Zhou et al., 2023b, Lemma 2)</ref>). For any policy 𝜋, and non-negative function 𝑔(𝑠, 𝑎), we have: where 𝜇 0 denotes the initial state distribution (which is the same for all policies 𝜋).</p><p>Proof. Recall that lim ℎ→∞ 𝛾 ℎ = 0. We start by noting that: where Line 13 follows by plugging in the relation in Line 12 for s, ā. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sorry to hear that you are feeling unmotivated. Here are some tips to help you regain some motivations: 1, Set Small, Achievable Goals. Try breaking down your work into small more manageable tasks. 2, Change Your Routine. Monotony can lead to demotivation. 3, Seek Inspiration. Try listening to an inspiring podcast, reading a motivational book, or talking to someone you find inspiring. 4, Take Care of Your Health. Lack of motivation can sometimes be linked to physical factors such as poor diet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Single-turn RL vs multi-turn RL for LLMs (ours). In particular, the single-turn RL agent seeks to resolve the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic of the practical instantiation of Actor-Critic Framework with a Hierarchical Structure (ArCHer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Webshop results. Observe that fine-tuning a GPT2 base model with ArCHer outperforms prior approaches, filtered BC and CHAI, and is the only approach to outperform GPT 3.5 equipped with several effective prompting strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) Ablation study of the token-level baseline on Guess My City. (b) Ablation study of the importance of off-policy data by varying the size of the replay buffer on Guess My City. (c) Ablation study of changing the base model for critic from encoder-only RoBERTa to autoregressive decoder-only GPT2 on Twenty Questions. (d) Ablation study of scaling the base model for the actor from GPT2 to Mistral7B on Twenty Questions Subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example actions and observations for each environment that we study in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A zoom-in of learning curves of PPO. PPO gradually improves despite its worse sample complexity compared to other off-policy methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $10.99 [B078GTKVXY] Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $10.99 [B08KBVJ4XN] Barrel and Oak -Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &amp; Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack) $15.95 Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J first.] Observation: OK. Action: click[B078GWRC1J] Observation: [Back to Search] [&lt; Prev] scent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents] size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)] Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3[For 3 ounce bottle of bright citrus deodorant for sensitive skin, the item has options 'bright citrus' and '3 ounce (pack of 1)' and seems good to buy.] Observation: OK. Action: click[bright citrus] Observation: You have clicked bright citrus. Action: click[3 ounce (pack of 1)] Observation: You have clicked 3 ounce (pack of 1). Action: click[Buy Now]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Prompts used by the act-only (expert) baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Prompts used by the act-only (expert) baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Ablation study of replacing ArCHer critic update with MC Regression. As data in the replay buffer is generated by different policies, MC regression fails to improve reliably. Ablation study of replacing ArCHer critic update with an IQL loss. Using an IQL critic loss can sometimes accelerate training but introduce more instability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎))where 𝜖 𝑠𝑡𝑎𝑡 is defined in Line 11. Finally, we can directly bound the errors for the advantages:E 𝑠,𝑎∼𝑑 𝜋 [(( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) f (𝑠, 𝑎)) − 𝐴 𝜋 (𝑠, 𝑎)) 2 ] =E 𝑠,𝑎∼𝑑 𝜋 [(( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) f (𝑠, 𝑎)) − (𝑄 𝜋 (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠)𝑄 𝜋 (𝑠,𝑎) )) 2 ] ≤2E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ] + 2E 𝑠∼𝑑 𝜋 [(E 𝑎 ′ ∼𝜋(•|𝑠) [ f (𝑠, 𝑎 ′ ) − 𝑄 𝜋 (𝑠, 𝑎 ′ )]) 2 ] ≤4E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ]inequality follows from Jensen's inequality, and the fourth inequality follows from Lemma 4. A similar analysis can be done with token-level FPE, noticing that the effective number of samples for token-level FPE is 𝑁 𝐿 because each utterance transition can be splitted into 𝐿 token transitions:E 𝑠,𝑎∼ ̃︀ 𝑑 𝜋 [(( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) f (𝑠, 𝑎)) − ̃︀ 𝐴 𝜋 (𝑠, 𝑎)) 2 ] ≤ 4 1 − 𝛾 (𝜖 𝑠𝑡𝑎𝑡 𝐿 −1/2 + 2(𝑅 + 1) √ 𝜖 𝑠𝑡𝑎𝑡 𝐿 −1/4 ). ≤ 4 (1 − 𝛾)𝐿 1/2 (𝜖 𝑠𝑡𝑎𝑡 + 2( 1 1 − 𝛾 + 1) √ 𝜖 𝑠𝑡𝑎𝑡 𝐿 1/4 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>E s,ā∼𝑑 𝜋 E 𝑠∼𝑃 (•|s,ā),𝑎∼𝜋(𝑎|𝑠) [𝑔(𝑠, 𝑎)] ≤ 1 𝛾 E 𝑠,𝑎∼𝑑 𝜋 [𝑔(𝑠, 𝑎)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>𝑑(</head><label></label><figDesc>𝜋 (𝑠, 𝑎) = (1 − 𝛾)(𝜇 0 (𝑠, 𝑎) + 𝛾𝑑 𝜋 1 (𝑠, 𝑎) + 𝛾 2 𝑑 𝜋 2 (𝑠, 𝑎) + . . . ) 𝜇 0 (s, ā) + 𝛾𝑑 𝜋 1 (s, ā) + . . . ) 𝑃 (𝑠|s, ā)𝜋(𝑎|𝑠) = 𝛾 ∑︁ s,ā 𝑑 𝜋 (s, ā)𝑃 (𝑠|s, ā)𝜋(𝑎|𝑠)(13)= 𝛾E s,ā∼𝑑 𝜋 [𝑃 (𝑠|s, ā)𝜋(𝑎|𝑠)],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Execute 𝑎 𝑡 ∼ 𝜋 𝜑 (•|𝑠 𝑡 ) , obtain the next state 𝑠 𝑡+1 , add to buffer 𝒟.</figDesc><table><row><cell>4:</cell><cell>## Data Collection.</cell><cell>◁ [only online mode]</cell></row><row><cell>5:</cell><cell>for each environment step do</cell><cell></cell></row><row><cell>6:</cell><cell></cell><cell></cell></row><row><cell>7:</cell><cell>end for</cell><cell></cell></row><row><cell>8:</cell><cell></cell><cell></cell></row></table><note>Algorithm 1 ArCHer: Practical Framework 1: Initialize parameters 𝜑, 𝜓, 𝜃, θ, (Optionally) 𝜂 2: Initialize replay buffer 𝒟 (optionally from an offline dataset). 3: for each iteration do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Variants of ArCHer in the offline RL setting. The performance of different approaches is evaluated by running 1280 trajectories across 5 random seeds for ArCHer and the Filtered BC approach. Observe that handling out-of-distribution actions by instantiating ArCHer with the IQL and AWR objectives works best in the offline setting.</figDesc><table><row><cell>Twenty Questions (Return)</cell></row></table><note>to an autoregressive decoder-only GPT2(Radford et al.,   </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Is the city you are from LondonIs the city you are from London Is the city you are from London Yes, my city is from London. Is the city you are from LondonIs the city you are from London Is the city you are from London Yes, my city is from London. Is the city you are from LondonIs the city you are from London Is the city you are from London Yes, my city is from London. Is the city you are from LondonIs the city you are from London Is the city you are from London Yes, my city is from London. Is the city you are from LondonIs the city you are from London Is the city you are from London Yes, my city is from London. ...... Example environment interactions of ArCHer for Guess My City at the end of training.The agent learns to ask the question for the predominant language, which is one of the most informative questions from the offline SFT distribution. However, as the oracle model provides free-form responses can be out of distribution, the agent is more prone to collapsing in out-of-distribution states. Figure 14: Example environment interactions of ArCHer for WebShop at the end of training. It can be seen that in this environment, the behavior of ArCHer is very natural.</figDesc><table><row><cell>Is the city you are from Shanghai? No, I am not from Shanghai. Is the city you are from Taipei,Is the city you are from TaipeiIs the city you are from Tis aipeiIs the city you are from Tai Yes Questions: What is the predominant language spoken: English Is the city you are from London? No, I am not from London. Observation: WebShop Instruction: i would like a citrus yao conditioner made with natural ingredients, and price lower than 50.00 dollars [Search] Action: search[citrus yao conditioner natural ingredients] Observation: [Back to Search] Page 1 (Total results: 50) [Next &gt;] [B099KWBV49] VIORI Terrace Garden Conditioner Bar -Handcrafted with Longsheng Rice Water &amp; Natural Ingredients -Sulfate-free, Paraben-free, Cruelty-free, Phthalate-free, pH balanced 100% Vegan, Zero-Waste $16.25 [B09KQZ9GTK] VIORI Hidden Waterfall Shampoo &amp; Conditioner Bar Set WITHOUT Bamboo Holder -Handcrafted with Longsheng Rice Water &amp; Natural Ingredients -Sulfate-free, Paraben-free, Phthalate-free, 100% Vegan $29.25 Action: click[B099KWBV49] Observation: [Back to Search] [&lt; Prev] style [bamboo bar holder][citrus yao][hidden waterfall] size [bamboo bar holder][conditioner][shampoo][shampoo + conditioner set] VIORI Terrace Garden Conditioner Bar -Handcrafted with Longsheng Rice Water &amp; Natural Ingredients -Sulfate-free, Paraben-free, Cruelty-free, Phthalate-free, pH balanced 100% Figure 13: WebShop Vegan, Zero-Waste</cell></row><row><cell>Price: $16.25</cell></row><row><cell>Rating: N.A.</cell></row><row><cell>[Description]</cell></row><row><cell>[Features]</cell></row><row><cell>[Reviews]</cell></row><row><cell>[Attributes]</cell></row><row><cell>[Buy Now]</cell></row><row><cell>Action:</cell></row><row><cell>click[citrus yao]</cell></row><row><cell>Observation:</cell></row><row><cell>You have clicked citrus yao.</cell></row><row><cell>Action:</cell></row><row><cell>click[Buy Now]</cell></row><row><cell>Observation:</cell></row><row><cell>Your score (min 0.0, max 1.0): 0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameters for All Experiments</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Detective Game Twenty Questions Subset Twenty Questions Guess My City Web Shopping</cell></row><row><cell>SFT</cell><cell>actor lr batch size</cell><cell>2e-4 32</cell><cell>2e-4 32</cell><cell>2e-4 32</cell><cell>2e-4 32</cell><cell>2e-4 32</cell></row><row><cell></cell><cell>actor lr</cell><cell>3e-4</cell><cell>3e-4</cell><cell>3e-5</cell><cell>3e-4</cell><cell>3e-4</cell></row><row><cell></cell><cell>batch size</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>Filtered</cell><cell>rollout trajectories</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>128</cell></row><row><cell>BC</cell><cell>replay buffer size</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell><cell>10000</cell></row><row><cell></cell><cell>filtering percentage</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell>actor updates per iteration</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell></cell><cell>actor lr</cell><cell>3e-4</cell><cell>3e-4</cell><cell>3e-5</cell><cell>3e-4</cell><cell>3e-4</cell></row><row><cell></cell><cell>critic lr</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell></row><row><cell></cell><cell>batch size</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>CHAI</cell><cell>rollout trajectories replay buffer size</cell><cell>128 10000</cell><cell>128 10000</cell><cell>128 10000</cell><cell>128 10000</cell><cell>512 10000</cell></row><row><cell></cell><cell>critic updates per iteration</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell></cell><cell>discount</cell><cell>0.98</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell><cell>0.9</cell></row><row><cell></cell><cell>polyak alpha</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell></cell><cell>actor lr</cell><cell>3e-4</cell><cell>3e-4</cell><cell>3e-5</cell><cell>3e-4</cell><cell>3e-4</cell></row><row><cell></cell><cell>critic lr</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell><cell>6e-4</cell></row><row><cell></cell><cell>batch size</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell></cell><cell>rollout trajectories</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>512</cell></row><row><cell>ArCHer</cell><cell>replay buffer size critic updates per iteration</cell><cell>10000 50</cell><cell>10000 50</cell><cell>10000 50</cell><cell>10000 50</cell><cell>10000 50</cell></row><row><cell></cell><cell>discount</cell><cell>0.98</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell><cell>0.9</cell></row><row><cell></cell><cell>polyak alpha</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell></cell><cell>actor updates per iteration</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell>warm up iters with no actor update</cell><cell>10</cell><cell>10</cell><cell>20</cell><cell>10</cell><cell>20</cell></row><row><cell></cell><cell>actor lr</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>3e-4</cell><cell>\</cell></row><row><cell></cell><cell>critic lr</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>6e-4</cell><cell>\</cell></row><row><cell></cell><cell>batch size</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>256</cell><cell>\</cell></row><row><cell>ArCHer w/ Baseline</cell><cell>rollout trajectories replay buffer size critic updates per iteration discount</cell><cell>\ \ \ \</cell><cell>\ \ \ \</cell><cell>\ \ \ \</cell><cell>128 10000 50 0.95</cell><cell>\ \ \ \</cell></row><row><cell></cell><cell>actor updates per iteration</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>3</cell><cell>\</cell></row><row><cell></cell><cell>baseline updates per iteration</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>60</cell><cell>\</cell></row><row><cell></cell><cell>warm up iters with no actor update</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>10</cell><cell>\</cell></row><row><cell></cell><cell>polyak alpha</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>0.9</cell><cell>\</cell></row><row><cell></cell><cell>actor lr</cell><cell>\</cell><cell>6e-6</cell><cell>6e-6</cell><cell>6e-4</cell><cell>\</cell></row><row><cell></cell><cell>batch size</cell><cell>\</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>\</cell></row><row><cell></cell><cell>rollout trajectories</cell><cell>\</cell><cell>2048</cell><cell>2048</cell><cell>1024</cell><cell>\</cell></row><row><cell></cell><cell>PPO epochs</cell><cell>\</cell><cell>10</cell><cell>20</cell><cell>4</cell><cell>\</cell></row><row><cell>PPO</cell><cell>discount</cell><cell>\</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell><cell>\</cell></row><row><cell></cell><cell>GAE lambda</cell><cell>\</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell><cell>\</cell></row><row><cell></cell><cell>clip range</cell><cell>\</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>\</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters for ArCHer and baseline methods for all experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Policy 𝜋, function class ℱ, number of iterations 𝐾, weight 𝜆 Require: 𝐾 independent datasets 𝒟 1:𝐾 = {(𝑠, 𝑎, 𝑟, 𝑠 ′ )} of 𝑀 many samples each from the same offline distribution 𝜈. 1: Initialize 𝑓 0 ∈ ℱ. 2: for 𝑘 = 1, . . . , 𝐾 do</figDesc><table><row><cell>Algorithm 2 Fitted Policy Evaluation (FPE)</cell><cell></cell><cell></cell></row><row><cell>Require: 3:</cell><cell></cell><cell></cell></row><row><cell>𝑠,𝑎</cell><cell>𝑑 𝜋 (𝑠, 𝑎) 𝜈(𝑠, 𝑎)</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The proof is constructed by noticing the fact that each token-level state consists of not only the utterance-level state but also all the tokens that have been generated in the current utterance:</figDesc><table><row><cell>Lemma 2. For any stationary policy 𝜋, and any offline distribution 𝜈, we have Density Ratio Coefficient for token-level ̃︁ ℳ = Density Ratio Coefficient for utterance-level ℳ. 𝐶 𝜈,𝜋 = 𝐶 ̃︀ 𝜈,𝜋 ︀ 𝑠∈ ̃︀ 𝒮,̃︀ 𝑎∈ ̃︀ 𝒜 ̃︀ 𝑑 𝜋 (̃︀ 𝑠, ̃︀ 𝑎) ︀ 𝜈(̃︀ 𝑠, ̃︀ 𝑎) = max 𝑠∈𝒮,̃︀ 𝑎 1:𝑖−1 ∈ ̃︀ 𝒜,𝑖∈[1,𝐿] ̃︀ 𝑑 𝜋 (𝑠, ̃︀ 𝑎 1:𝑖−1 , ̃︀ 𝑎 𝑖 ) ︀ 𝜈(𝑠, ̃︀ 𝑎 1:𝑖−1 , ̃︀ 𝑎 𝑖 ) = max 𝑠∈𝒮,̃︀ 𝑎 1:𝐿 ∈ ̃︀ 𝒜 ̃︀ 𝑑 𝜋 (𝑠, ̃︀ 𝑎 1:𝐿 ) ︀ 𝜈(𝑠, ̃︀ 𝑎 1:𝐿 ) = max 𝑠∈𝒮,𝑎∈𝒜 𝑑 𝜋 (𝑠, 𝑎) 𝜈(𝑠, 𝑎) , where the first equation follows by regrouping states and actions as each state in the token-level state Proof. max space ̃︀ 𝑠 ∈ ̃︀ 𝒮 = 𝒮 × ̃︀ 𝒜 𝐿 , and the second equation holds because max 𝑏 𝑝(𝑎,𝑏) 𝑞(𝑎,𝑏) ≥ 𝑝(𝑎) 𝑞(𝑎) .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>𝑎∼𝜈 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝐾 (𝑠, 𝑎) + 𝑎) − 𝒯 𝜋 𝑓 𝑘−1 (𝑠, 𝑎)) (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝐾 (𝑠, 𝑎)) 2 + 𝑘 (𝑠, 𝑎) − 𝒯 𝜋 𝑓 𝑘−1 (𝑠, 𝑎)) 2where the first inequality follows by Jensen's Inequality and the second inequality follows by plugging in Line 9. Then, we can plug in Assumption 2 to conclude the proof.E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] ≤𝐶 𝜈,𝜋 E 𝑠,𝑎∼𝜈 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ]Theorem 1 (Main Theorem). For an utterance-level MDP with discount factor 𝛾 𝐿 , where 𝐿 is the maximum length of each utterance, suppose utterance-level Assumption 1 and 2 holds, let 𝑓 be the final Q function returned by Fitted Policy Evaluation formalized in Algorithm 2 at the utterance level, yields a suboptimality gap ofE 𝑠,𝑎∼𝑑 𝜋 [(( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) [ f (𝑠, 𝑎)]) − 𝐴 𝜋 (𝑠, 𝑎)) 2 ]For an equivalent token-level MDP with discount factor 𝛾 1/𝐿 , suppose token-level Assumption 1 and 2 holds, let 𝑓 be the final Q function returned by Fitted Policy Evaluation formalized in Algorithm 2 at the token level, yields a suboptimality gap ofE 𝑠,𝑎∼𝑑 𝜋 [(( f (𝑠, 𝑎) − E 𝑎 ′ ∼𝜋(•|𝑠) [ f (𝑠, 𝑎)]) − ̃︀ 𝐴 𝜋 (𝑠, 𝑎)) 2 ]where 𝜖 𝑠𝑡𝑎𝑡 is the statistical error defined in Line 11 proportional to 𝑁 −1/2 the number of utterance-level transitions. This error term is defined the same for both utterance-level MDP and token-level MDP.Proof. First, we start with analyzing utterance-level FPE with effective number of samples, we start by bounding the errors of the Q functions:E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 =E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎) + 𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 =E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 + 2( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎))(𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) + (𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ] ≤E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] + 2 √︁ E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 (𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 + E 𝑠,𝑎∼𝑑 𝜋 [(𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ] ≤E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] + 2(𝑅 + 1) √︁ E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 + E 𝑠,𝑎∼𝑑 𝜋 [(𝒯 𝜋 f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ] ≤E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] + 2(𝑅 + 1) √︁ E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 + 𝛾 2 E 𝑠,𝑎∼𝑑 𝜋 ,𝑠 ′ ∼𝑃 (•|𝑠,𝑎),𝑎 ′ ∼𝜋(•|𝑠 ′ ) [( f (𝑠 ′ , 𝑎 ′ ) − 𝑄 𝜋 (𝑠 ′ , 𝑎 ′ )) 2 ] ≤E 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] + 2(𝑅 + 1) √︁ E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 + 𝛾E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 ,where the first inequality follows from Jensen's inequality, and the second inequality follows from max 𝑠,𝑎 max{|𝒯 𝜋 f (𝑠, 𝑎)|, |𝑄 𝜋 (𝑠, 𝑎)|} ≤ 𝑅 + 1, the third inequality follows again from Jensen's inequality,and the fourth inequality follows from Lemma 5. Finally, we get:E 𝑠,𝑎∼𝑑 𝜋 ( f (𝑠, 𝑎) − 𝑄 𝜋 (𝑠, 𝑎)) 2 𝑠,𝑎∼𝑑 𝜋 [( f (𝑠, 𝑎) − 𝒯 𝜋 f (𝑠, 𝑎)) 2 ] + 2(𝑅 + 1) √︁ E 𝑠,𝑎∼𝑑 𝜋</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>≤ 𝐶 𝜈,𝜋 (</cell><cell>4(𝑅 + 1) 2 𝐾</cell><cell>+</cell><cell>256𝑅 2 log(2|ℱ|𝐾/𝛿) 𝑀</cell><cell>).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>≤ 64𝐶 𝜈,𝜋 𝑅(𝑅 + 1)</cell><cell>√︂</cell><cell>log(2|ℱ|𝐾/𝛿) 𝐾𝑀</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 64𝐶 𝜈,𝜋 𝑅(𝑅 + 1)</cell><cell>√︂</cell><cell>log(2|ℱ|𝐾/𝛿) 𝑁</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:= 𝜖 𝑠𝑡𝑎𝑡</cell><cell>(11)</cell></row><row><cell>≤</cell><cell>1 1 − 𝛾</cell><cell>(E</cell><cell></cell><cell cols="2">≤ ≤</cell><cell cols="2">4 1 − 𝛾 𝛾𝐿(1 − 𝛾 1/𝐿 ) (𝜖 𝑠𝑡𝑎𝑡 + 2(𝑅 + 1) 4 (𝜖 𝑠𝑡𝑎𝑡 + 2( √ 1 − 𝛾 𝜖 𝑠𝑡𝑎𝑡 ) 1</cell><cell>+ 1)</cell><cell>𝜖 𝑠𝑡𝑎𝑡 ). √</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">≤</cell><cell cols="2">4 (1 − 𝛾 1/𝐿 )𝐿 1/2 (𝜖 𝑠𝑡𝑎𝑡 + 2(</cell><cell>1 1 − 𝛾</cell><cell>+ 1)</cell><cell>√</cell><cell>𝜖 𝑠𝑡𝑎𝑡 𝐿 1/4 ),</cell></row><row><cell></cell><cell cols="3">=E 𝑠,𝑎∼𝜈</cell><cell cols="3">⎡ ⎣ (︃</cell><cell>1 𝐾</cell><cell>(𝑓 1 )︃ 2 ⎤ ⎦</cell></row><row><cell></cell><cell>≤</cell><cell>1 𝐾</cell><cell cols="2">E 𝑠,𝑎∼𝜈</cell><cell cols="3">[︃</cell><cell>(𝑓 1 ]︃</cell></row><row><cell></cell><cell>≤</cell><cell>1 𝐾</cell><cell cols="5">[︂ 4(𝑅 + 1) 2 + (𝐾 − 1)</cell><cell>𝑀 256𝑅 2 log(2|ℱ|𝐾/𝛿)</cell><cell>]︂</cell></row><row><cell></cell><cell>≤</cell><cell cols="4">4(𝑅 + 1) 2 𝐾</cell><cell></cell><cell>+</cell><cell>256𝑅 2 log(2|ℱ|𝐾/𝛿) 𝑀</cell><cell>,</cell><cell>(10)</cell></row></table><note>𝐾 ∑︁ 𝑘=2 𝑓 𝑘 (𝑠, 𝐾 ∑︁ 𝑘=2 (𝑓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The above implies that for any function 𝑔 ≥ 0, which implies thatE s,ā∼𝑑 𝜋 E 𝑠∼𝑃 (•|s,ā),𝑎∼𝜋(𝑎|𝑠) [𝑔(𝑠, 𝑎)] ≤ 1 𝛾 E 𝑠,𝑎∼𝑑 𝜋 [𝑔(𝑠, 𝑎)].Lemma 6 (Least squares generalization bound,(Song et al., 2023, Lemma 3)).Let 𝑅 &gt; 0, 𝛿 ∈ (0,1), and consider a sequential function estimation setting with an instance space 𝒳 and target space 𝒴. Let ℋ : 𝒳 ↦ → [−𝑅, 𝑅] be a class of real valued functions. Let 𝒟 = {(𝑥 1 , 𝑦 1 ), . . . , (𝑥 𝑇 , 𝑦 𝑇 )} be a dataset of 𝑇 points where 𝑥 𝑡 ∼ 𝜌 𝑡 := 𝜌 𝑡 (𝑥 1:𝑡−1 , 𝑦 1:𝑡−1 ), and 𝑦 𝑡 is sampled via the conditional probability 𝑝 𝑡 (𝑥 𝑡 ) (which could be adversarially chosen). Additionally, suppose that max 𝑡 |𝑦 𝑡 | ≤ 𝑅 and max ℎ max 𝑥 |ℎ(𝑥)| ≤ 𝑅. Then, the least square solution ̂︀ ℎ ← argmin ℎ∈ℋ ∑︀ 𝑇 𝑡=1 (ℎ(𝑥 𝑡 ) − 𝑦 𝑡 ) 2 satisfies</figDesc><table><row><cell>∑︁</cell></row><row><cell>𝑠,𝑎</cell></row></table><note>𝑑 𝜋 (𝑠, 𝑎)𝑔(𝑠, 𝑎) ≥ ∑︁ 𝑠,𝑎 𝛾E s,ā∼𝑑 𝜋 [𝑃 (𝑠|s, ā)𝜋(𝑎|𝑠)𝑔(𝑠, 𝑎)], 𝑇 ∑︁ 𝑡=1 E 𝑥∼𝜌𝑡,𝑦∼𝑝𝑡(𝑥) [︁ ( ̂︀ ℎ(𝑥) − 𝑦) 2 ]︁ ≤ 3 inf ℎ∈ℋ 𝑇 ∑︁ 𝑡=1 E 𝑥∼𝜌𝑡,𝑦∼𝑝𝑡(𝑥) [︀ (ℎ(𝑥) − 𝑦) 2 ]︀ + 256𝑅 2 log(2|ℋ|/𝛿)with probability at least 1 − 𝛿.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For more details, see the Github Issue on ReAct's repository: https://github.com/ysymyth/ReAct</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Seohong Park, Mitsuhiko Nakamoto, Joey Hong, Yi Su, and members of the RAIL lab at UC Berkeley for valuable feedback on a previous version of this paper. YZ and AZ thank Marwa Abdulhai, Isadora White, and Charlie Snell for help with debugging and setting up the LMRL benchmark. AK thanks Charlie Snell, George Tucker, and Young Geng for informative discussions and perspectives. This work is supported by NSF IIS-2246811 and ONR N00014-21-1-2838, TRC TPU credit donations from Google Cloud, and compute credit donations from the Center for AI Safety (CAIS). AZ is supported by a postdoctoral fellowship from the Simons Institute for the Theory of Computing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>This work was done at UC Berkeley. Yifei Zhou led the project. He wrote the prototype of ArCHer, iterated on refining and improving it, implemented the baselines for online comparisons, and different ablation in the online experiments. He also took a lead in writing the paper. Andrea Zanette led the offline experiments of the project. He developed the offline variant of ArCHer with IQL and AWR, and helped editing the paper. Jiayi Pan set up the environment of webshop for our use and implemented prompting baselines. He also refactored the code and helped set up data distributed parallel training for ArCHer and baselines. Sergey Levine advised the project, provided inputs for prototyping the method, and helped editing the paper. Aviral Kumar proposed the project idea, advised the project, helped the prototyping of ArCHer, and substantially edited the paper.</p><p>tend to be natural and realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twenty Questions Subset</head><p>Questions: Is it a living thing? No. Is it a man-made? Yes. Is it a machine? No. Is it a tool? No. Is it a piece of clothing? No. Is it a piece of furniture? Yes. Is it a chair? Yes.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models</title>
		<author>
			<persName><forename type="first">Marwa</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isadora</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiwoz -A large-scale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName><surname>Gasic</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00278</idno>
		<ptr target="http://arxiv.org/abs/1810.00278" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xander</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Krendl</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Scheurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Rando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charbel-Raphaël</forename><surname>Segerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Christoffersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehul</forename><surname>Damani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Slocum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usman</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Siththaranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Krasheninnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<editor>Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>Lauro Langosco, Peter Hase</pubPlace>
		</imprint>
	</monogr>
	<note>Open problems and fundamental limitations of reinforcement learning from human feedback</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fireact: Toward language agent fine-tuning</title>
		<author>
			<persName><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<idno>ArXiv, abs/2310.05915</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:263829338" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from human preferences</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<editor>Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Scaling instruction-finetuned language models</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Fundamental barriers for value function approximation</title>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simchi-Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzong</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/2111.10919</idno>
		<ptr target="https://arxiv.org/abs/2111.10919" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A minimalist approach to offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Shane Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling laws for reward model overoptimization</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10835" to="10866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cicero: A dataset for contextualized commonsense inference in dialogues</title>
		<author>
			<persName><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">247762111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Trebacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Thacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Campbell-Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramona</forename><surname>Comanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Sanchez Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soňa</forename><surname>Mokrá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Improving alignment of dialogue agents via targeted human judgements</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reinforced self-training (rest) for language modeling</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srivatsan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ksenia</forename><surname>Konyushkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lotte</forename><surname>Weerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaosen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08998</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1801.01290</idno>
		<ptr target="http://arxiv.org/abs/1801.01290" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Interactive fiction games: A colossal adventure</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<idno>CoRR, abs/1909.05398</idno>
		<ptr target="http://arxiv.org/abs/1909.05398" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Zero-shot goal-directed dialogue via rl on imagined conversations</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GPT-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Youngsoo</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qaxhBG1UUaS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human-centric dialog training via offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">Hanwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Shane Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<idno>CoRR, abs/2010.05848</idno>
		<ptr target="https://arxiv.org/abs/2010.05848" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
		</author>
		<title level="m">Mistral 7b</title>
				<editor>
			<persName><forename type="first">Marie-Anne</forename><surname>Lavaud</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre</forename><surname>Lachaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teven</forename><surname>Stock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibaut</forename><surname>Le Scao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Lavril</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothée</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">El</forename><surname>Lacroix</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sayed</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pretraining language models with human preferences</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stabilizing off-policy q-learning via bootstrapping error reduction</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine ; Aviral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2021. 2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Offline reinforcement learning with implicit q-learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conservative q-learning for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine ; Sungjin Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.04779" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006">CoRR, abs/2006.04779. 2020. 2019</date>
		</imprint>
	</monogr>
	<note>Convlab: Multi-domain end-to-end dialog system platform</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Jaymin</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Sutherland Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:246527904" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<idno>CoRR, abs/1802.08979</idno>
		<ptr target="http://arxiv.org/abs/1802.08979" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangliang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shudan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>CoRR, abs/1312.5602</idno>
		<ptr target="http://arxiv.org/abs/1312.5602" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The primacy bias in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Nikishin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Pierluca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16828" to="16847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">:</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Anadkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Red</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Balcom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiming</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Belgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Berdine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bernadett-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenny</forename><surname>Bogdonoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Boiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelaine</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna-Luisa</forename><surname>Brakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosie</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brittany</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotis</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sully</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruby</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Currier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Decareaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Degry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Deville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atty</forename><surname>Eleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simón</forename><surname>Posada Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juston</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Fulford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vik</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarun</forename><surname>Gogineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rapha</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Grafstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Heidecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Hoeschele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanne</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shino</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billie</forename><surname>Jomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Kanitscheider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tabarak</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kilpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kokotajlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kondraciuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aris</forename><surname>Kondrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Kosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ikai</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molly</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Lue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Makanju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Malfacini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Markovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><surname>Mayne ; Aalok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinnie</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Monaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Mossing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Murk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonwoo</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Cullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Pantuliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emy</forename><surname>Parish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Parparita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Perelman ; John Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyla</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toki</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Sherbakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shoker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Sitkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Sokolowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Staudacher</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akila</forename><surname>Cj Weinmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welihinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wiethoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Willner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Wolrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zhuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Felipe Petroski Such</title>
		<editor>Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Amin Tootoonchian</publisher>
		</imprint>
		<respStmt>
			<orgName>Filipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya</orgName>
		</respStmt>
	</monogr>
	<note>Gpt-4 technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Francis Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Lowe</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.02155</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:246426909" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Advantage-weighted regression: Simple and scalable off-policy reinforcement learning</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1910.00177</idno>
		<ptr target="http://arxiv.org/abs/1910.00177" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">160025533</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization</title>
		<author>
			<persName><forename type="first">Rajkumar</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kianté</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafet</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.01241" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization</title>
		<author>
			<persName><forename type="first">Rajkumar</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kianté</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafet</forename><surname>Sifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Toolformer: Language models can teach themselves to use tools</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dessì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR, abs/1506.02438</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:3075448" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>CoRR, abs/1707.06347</idno>
		<ptr target="http://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reflexion: Language agents with verbal reinforcement learning</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beck</forename><surname>Labash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">258833055</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Offline rl for natural language generation with implicit language q learning</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hybrid rl: Using both offline and online data can make rl efficient</title>
		<author>
			<persName><forename type="first">Yuda</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Sekhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Large language models as generalizable policies for embodied tasks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Mackraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xiang Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iliyan</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Aurelien Rodriguez</publisher>
			<pubPlace>Angela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunov</pubPlace>
		</imprint>
	</monogr>
	<note>and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><surname>Silver</surname></persName>
		</author>
		<idno>CoRR, abs/1509.06461</idno>
		<ptr target="http://arxiv.org/abs/1509.06461" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models</title>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.16291</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:258887849" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:19115634" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning to extract coherent summary via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bellman-consistent pessimism for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-An</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<idno>CoRR, abs/2106.06926</idno>
		<ptr target="https://arxiv.org/abs/2106.06926" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Intercode: Standardizing and benchmarking interactive coding with execution feedback</title>
		<author>
			<persName><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshara</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">M</forename><surname>Swope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Chalamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15626</idno>
		<title level="m">LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</title>
				<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Webshop: Towards scalable real-world web interaction with grounded language agents</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rrhf: Rank responses to align language models with human feedback without tears</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">When is realizability sufficient for off-policy reinforcement learning?</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zanette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Agenttuning: Enabling generalized agent abilities for llms</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingdao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning with realizability and single-policy concentrability</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baihe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Webarena: A realistic web environment for building autonomous agents</title>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abishek</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>ArXiv, abs/2307.13854</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:260164780" />
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Offline data enhanced on-policy policy gradient with provable guarantees</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Sekhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuda</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Convlab-2: An open-source toolkit for building, evaluating, and diagnosing dialogue systems</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/2002.04793</idno>
		<ptr target="https://arxiv.org/abs/2002.04793" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Fine-tuning language models from human preferences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08593</idno>
		<ptr target="http://arxiv.org/abs/1909.08593" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
